{"meta":{"title":"苇名一心的博客","subtitle":null,"description":"经验分享和技术交流。","author":"苇名一心","url":"https://666wxy666.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-04-01T09:45:17.617Z","updated":"2020-03-25T08:57:28.904Z","comments":true,"path":"404.html","permalink":"https://666wxy666.github.io/404.html","excerpt":"","text":"404很抱歉，您访问的页面不存在可能是输入地址有误或该地址已被删除，您可以留言告诉我您要访问哪个页面找不到了，谢谢。"},{"title":"关于","date":"2020-04-27T12:38:07.457Z","updated":"2020-04-27T12:38:07.457Z","comments":true,"path":"about/index.html","permalink":"https://666wxy666.github.io/about/index.html","excerpt":"","text":"这里是我的基本信息：我的博客（Github）：苇名一心我的博客（Gitee）：苇名一心我的Github主页：666WXY666我的Gitee主页：wxy_666我的B站：在下-苇名一心我的知乎：苇名一心我的网易云：在下-苇名一心我的微博：在下-苇名一心欢迎关注我，与我联系。"},{"title":"所有分类","date":"2020-04-01T09:45:17.618Z","updated":"2020-03-22T10:40:41.483Z","comments":true,"path":"categories/index.html","permalink":"https://666wxy666.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-04-01T09:45:17.618Z","updated":"2020-03-25T07:29:48.422Z","comments":true,"path":"faqs/index.html","permalink":"https://666wxy666.github.io/faqs/index.html","excerpt":"","text":"README"},{"title":"","date":"2020-04-01T09:45:17.618Z","updated":"2020-03-22T01:11:48.329Z","comments":true,"path":"mylist/index.html","permalink":"https://666wxy666.github.io/mylist/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2020-04-07T12:01:17.904Z","updated":"2020-04-07T12:01:17.904Z","comments":true,"path":"friends/index.html","permalink":"https://666wxy666.github.io/friends/index.html","excerpt":"这里是我的朋友和一些大佬，感兴趣的就点进去看看吧！","text":"这里是我的朋友和一些大佬，感兴趣的就点进去看看吧！想在这里展示或者有什么问题欢迎给我留言。"},{"title":"所有标签","date":"2020-04-01T09:45:17.619Z","updated":"2020-03-25T07:51:09.297Z","comments":true,"path":"tags/index.html","permalink":"https://666wxy666.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Linux MOOC习题 5~9章","slug":"Linux-MOOC习题-5~9章","date":"2020-04-29T01:54:47.814Z","updated":"2020-04-29T03:35:44.444Z","comments":true,"path":"2020/04/29/Linux-MOOC习题-5~9章/","link":"","permalink":"https://666wxy666.github.io/2020/04/29/Linux-MOOC%E4%B9%A0%E9%A2%98-5~9%E7%AB%A0/","excerpt":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的，接上文Linux MOOC习题 1~4章。","text":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的，接上文Linux MOOC习题 1~4章。PS：第七章是上机实验，第一次上机实验详见我的另一篇文章：Linux 上机实战1 正则表达式，第二次的实验因为是提交阶段暂不公开，后面可能会发。五、文件管理和目录管理 在Linux中，Shell会自动帮你进项一些替换，实际运行的命令其实是：1cp src/x.c src/xx.c src/xxx.c src/x.h src/xx.h src/xxx.h……如果有好多.c和.h文件（超过两个），cp就会报错：cp: target xxx is not a directory但是如果只有两个.c或.h文件，那么就会产生非常严重的后果，实际运行了下面的命令：1cp src/x.c src/y.c这是非常危险的，这样y.c的内容会被x.c覆盖，这是我们不愿意看到的。那么应该用什么命令来实现题目的要求呢？1cp src/*.[ch] .我们显示地指定复制到‘.’（当前目录），就不会出现问题了。这也是在Linux中很怪的地方，明明你不能写这个文件，但是你却可以删除，如果不加-f，删除只读文件时会有提示，但是加了-f就没有任何提示了。还有一点需要注意的是，如第6题所说，加了-f也不能删除无权限删除的文件。 关于第10题，不像Windows，在Linux中这些后缀都是约定俗成的，只是为了做标记用，好区分而已，没有实际意义。六、Linux命令风格和文件系统这两个题都忽略了“符号链接”的作用。 这俩没啥好说的。在Linux中，一般是-1代表失败，&gt;=0的值代表成功。八、文件和目录的权限、Shell目录有执行权限（即x权限）意味着分析路径名过程中可检索该目录。九、替换、元字符和转义如果你是执行这个命令的操作员，估计你马上就心情不好了。由于拼错了单词，把DATABASE不小心写成了DATEBASE，灾难来了。未命名的变量被bash替换为空字符串，实际上你以root身份执行了最邪恶的一条命令：1rm -rf /*永远不要盲目自信，谁也保证不了自己不会犯错误，怎么才能避免这样的悲剧发生？尽量不要以root身份登录。bash有选项，引用未定义的变量会出错而不是替换为空字符串，可以打开这个选项。早期的编程语言把引用的未定义过的变量自动加上定义，这种做法实际上太糟糕，最早的FORTRAN语言就这种做法，据说曾因此导致一次太空任务失败。在设计你自己的系统时，建库命令能够从目录名开始建库，就是说要求建库之前/opt/data下不需要存在目录puma，而不是要求建库之前/opt/data有个空目录puma，这样的话，即使你的命令变成了1rm -rf $DATEBASE因为你的失误，会导致rm命令抱怨缺少参数而什么都不做，这个结果是可以接受的。在Linux中，目前我已知的Shell的元字符有：1空格 制表符 回车 &gt; &lt; | ; &amp; $ * [ ] ? \\ ( ) '' \" \" 反撇号`第11题中，这三个命令都可以取消文件通配符*的特殊作用，让echo直接打印字符*，并且这个替换是Shell进行的，也就是说，echo拿到的命令都是一样的，他是分不清操作员输入的是哪个命令，这三个命令的参数：而对于不转义的命令的参数：1echo *因此结果就很明显了：未完待续……所有的习题：Linux MOOC习题 1~4章Linux MOOC习题 5~9章","categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"习题","slug":"Linux/习题","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B9%A0%E9%A2%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"习题","slug":"习题","permalink":"https://666wxy666.github.io/tags/%E4%B9%A0%E9%A2%98/"},{"name":"易错点","slug":"易错点","permalink":"https://666wxy666.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"网课","slug":"网课","permalink":"https://666wxy666.github.io/tags/%E7%BD%91%E8%AF%BE/"},{"name":"MOOC","slug":"MOOC","permalink":"https://666wxy666.github.io/tags/MOOC/"}]},{"title":"Linux MOOC习题 1~4章","slug":"Linux-MOOC习题-1~4章","date":"2020-04-28T13:08:08.946Z","updated":"2020-04-29T03:33:53.215Z","comments":true,"path":"2020/04/28/Linux-MOOC习题-1~4章/","link":"","permalink":"https://666wxy666.github.io/2020/04/28/Linux-MOOC%E4%B9%A0%E9%A2%98-1~4%E7%AB%A0/","excerpt":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的","text":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的废话不多说，直接开始：PS：第一章是课程介绍。二、开始使用Linux和文本文件的处理第1题其实是很基础的问题，早期的终端一般由键盘、显示器和RS232串行通信接口构成，没有磁盘存储器，其实可以想象成和打字机差不多的东西，他就是直接在显示器上显示了，不需要磁盘存储器。第2题也比较基础：行律的作用是：一行内字符的缓冲、回显和编辑，直到按下回车键；数据的加工，类似第二题中的将“\\n”替换为“\\r\\n”；将CTRL-C字符转换为终止进程的信号；驱动程序其实是串口与行律的接口，负责上行和下行字符流。这俩题没啥好说的，基础知识。并不是这样的，CTRL-C确实传送了字符（Ctrl+字母组合键可以产生ASCII码为1-26的控制字符，字母序号是几，ASCII码就是几，Ctrl+C的ASCII码应为3），其实还是行律的作用，他将CTRL-C字符转换为终止进程的信号，从而通知Linux主机，进程终止。 这俩题都是关于uniq命令的。123uniq -uuniq -duniq第一个命令，-u代表unique，只打印没有重复的行；第二个命令，-d代表duplicated，只打印重复的行，注意重复的行只打印一次；第三个命令，啥也不加就是都打印，但是也是复的行只打印一次；还有就是关于重复的行，意思其实是连续的紧邻的两行内容相同才被叫做重复的行，因此第10题是错的。这个题不难，写出来的目的是只要记得Less is more就很好想了，less是more的升级版。 这俩题也不多解释了，od不可打印字符也可以显示，tr是用于翻译，把string1出现的字符替换为string2中对应的字符，ASCII字符0也可以翻译。1tr string1 string2 三、正则表达式先说一下正则表达式的元字符，有6个，分别是：1. * [ \\ ^ $关于他们各自的含义不多说，自行百度，一定要注意的是]不是元字符，很容易搞错。而关于单字符正则表达式，显而易见，就是匹配一个字符呗，不过有特殊的:\\加一个字符构成的转义字符，看做单字符正则表达式；1\\. \\* \\[ \\\\ \\^ \\$[]定义的集合也被看做单字符正则表达式;因为.本来的含义是匹配任意字符，转义后就是单字符.，因此都是单字符正则表达式，而$在尾部时和^在首部时有特殊含义，因此转义之前不是，转义后就是单字符​了。这个题就比较有意思了，先看一下sed命令的基本用法：1sed 's/正则表达式/字符串/g' 文件名列表s选项的意思是替换，将文件中匹配第一个//中的正则表达式的内容替换为第二个//中的字符串。然后我们在看上题‘’中的正则表达式（也就是第一个//中的内容）1\\[[^][]*]可以分为三部分：因为[是元字符，加\\转义后就变成了真正的[。1\\[中间这一部分是一个集合[]，重复一次或多次[]*，集合里面的^代表排除，排除了]和[，意思就是说除]和[外的字符重复一次或多次。那么这就有一个问题，第一个[为什么不与第一个]匹配，反而去和最后一个]匹配呢？个人觉得，因为如果与第一个]匹配，就有了[^]，这显然是错误的正则表达式，那么就继续向后匹配，因此就匹配到了最后面的]。1[^][]*因为]不是元字符，也没有[与它匹配，它就是单纯的一个字符]。1]综上所述，这个正则表达式可以匹配“[非]和[的字符任意多个]”，类似[参考文献23]这样的，但是如果[]里面还有]和[，就匹配不到了。再看上面的题，第二个//里面是空的，那么就是把匹配到的内容删除，很好理解。四、文件比较，文件通配符vi的基本用法，在命令状态12345678:q 退出（:quit的缩写）:q! 退出且不保存（:quit!的缩写）:wq 保存并退出:wq! 保存并退出即使文件没有写入权限（强制保存退出）:x 保存并退出（类似:wq，但是只有在有更改的情况下才保存）:exit 保存并退出（和:x相同）:qa 退出所有(:quitall的缩写):cq 退出且不保存（即便有错误）另外在“正常模式”下输入“ZZ”来保存并退出Vim（和:x相同），或者“ZQ”不保存并退出（和:q!相同）注意此处ZZ大写和小写是完全不同的。diff一般用于比较文本文件。未完待续……所有的习题：Linux MOOC习题 1~4章Linux MOOC习题 5~9章","categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"习题","slug":"Linux/习题","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B9%A0%E9%A2%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"习题","slug":"习题","permalink":"https://666wxy666.github.io/tags/%E4%B9%A0%E9%A2%98/"},{"name":"易错点","slug":"易错点","permalink":"https://666wxy666.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"网课","slug":"网课","permalink":"https://666wxy666.github.io/tags/%E7%BD%91%E8%AF%BE/"},{"name":"MOOC","slug":"MOOC","permalink":"https://666wxy666.github.io/tags/MOOC/"}]},{"title":"Python爬虫小Demo：学堂在线课程","slug":"Python爬虫小Demo","date":"2020-04-07T11:43:33.300Z","updated":"2020-04-08T14:42:43.756Z","comments":true,"path":"2020/04/07/Python爬虫小Demo/","link":"","permalink":"https://666wxy666.github.io/2020/04/07/Python%E7%88%AC%E8%99%AB%E5%B0%8FDemo/","excerpt":"本文是关于Python中的Spider的小Demo，通过Python的scrapy爬取京学堂在线课程的相关数据。","text":"本文是关于Python中的Spider的小Demo，通过Python的scrapy爬取京学堂在线课程的相关数据。一、编译环境：PyCharm 2019.3.4 (Professional Edition)Build #PY-193.6911.25, built on March 18, 2020Runtime version: 11.0.6+8-b520.43 amd64VM: OpenJDK 64-Bit Server VM by JetBrains s.r.oWindows 10 10.0GC: ParNew, ConcurrentMarkSweepMemory: 725MCores: 8Registry: ide.balloon.shadow.size=0Non-Bundled Plugins:GrepConsole,Statistic,cn.yiiguxing.plugin.translate,com.chrisrm.idea.MaterialThemeUI,com.notime.intellijPlugin.backgroundImagePlus,com.wakatime.intellij.plugin,izhangzhihao.rainbow.brackets,mobi.hsz.idea.gitignore, net.vektah.codeglance, org.intellij.giteePython Version：3.7（Anaconda3）Package：scrapy==2.0.1二、详细步骤①准备工作在Pycharm中新建一个Pure Python项目（记得要按照一中的要求配好Python环境）。打开Pycharm的下方的终端（Terminal），当然这些也可以在系统终端里操作，不过可能需要的步骤多一些，还是直接在Pycharm里方便一些。在终端里输入1scrapy startproject myScrapy本来是可以在我们刚刚创建的项目里新建一个名为“myScrapy”的scrapy项目的，但是不知道为什么竟然报错了：这是什么奇奇怪怪的错误，我都没有“d:\\bld\\scrapy_1584555997548_h_env\\python.exe”这个目录，经过查找相关问题的资料，问题可能是出在Python环境上，因为我上一次新建项目时并没有报错，这一次我直接用的上一个项目的环境，网上还有一种说法是Scrapy的bug，详见Fatal error launching scrapy&gt;1.6.0 from Anaconda PromptIssue with conda-forge scrapy&gt;1.6.0 on Windows目前找到了两种解决方案：把python环境复制到报错的那个目录（d:\\bld\\scrapy_1584555997548_h_env\\python.exe），然后在创建scrapy项目，但这个解决方法有点愚蠢，就没有采用。在scrapy命令前面添加“python -m”选项：1python -m scrapy startproject myScrapy就可以正常创建了。关于Python的-m选项，官方给出的解释是“run library module as a script”，简单来说就是将库中的Python模块当作脚本去运行。特别感谢简书的大佬ccw1078提供的解释，很清晰明了，有兴趣的可以去瞅一下，因为和本文的主题爬虫没啥关系，在这里就不赘述了。出现这些提示就代表创建成功了。然后在spiders文件里新建一个spider.py文件，用于写爬虫。我们来看一下目前scrapy项目的目录结构。__init__.py：pycharm生成的文件，简化导入语句用的，可以忽略，没啥用，建议删了，留着可能会出问题。spiders：存放你Spider爬虫源文件​ spider.py：代码主要在这里写。items.py：数据容器。middlewares.py：Downloader Middlewares(下载器中间件)和Spider Middlewares(蜘蛛中间件)实现的地方。pipelines.py：项目管道文件，相当于数据中转站。实现数据的清洗，储存，验证。settings.py：scrapy的全局配置。scrapy.cfg：配置文件。scrapy已经帮我们把大体框架写好了，我们主要要修改的文件是spider.py，items.py，pipelines.py，settings.py。这是爬虫spider的基本工作方式，想要深入了解的可以去网上查找资料。②开始写代码先来写items.py。123456789import scrapyclass MyscrapyItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() school = scrapy.Field() num = scrapy.Field() pass再来搞settings.py，这个只需要找到这个注释掉的语句，把#去掉就OK了，就像这样：pipelines.py就很好写了，基本可以当模板来用。12345678910111213141516171819import jsonclass MyscrapyPipeline(object): def open_spider(self, spider): try: # 这个就是爬虫生成的文件，可以支持好多种格式，这里使用的是json文件 self.file = open('MyData.json', 'w', encoding=\"utf-8\") except Exception as err: print(err) def process_item(self, item, spider): dict_item = dict(item) json_str = json.dumps(dict_item, ensure_ascii=False) + \"\\n\" self.file.write(json_str) return item def close_spider(self, spider): self.file.close()spider.py是我们主要写的核心部分。这里需要一些html的xpath相关知识来对项进行定位，可以自行查找相关资料。12345678910111213141516171819202122232425262728293031323334353637383940414243\"\"\"@Copyright: Copyright (c) 2020 苇名一心 All Rights Reserved.@Project: xuetangzaixian@Description: @Version: @Author: 苇名一心@Date: 2020-04-08 20:31@LastEditors: 苇名一心@LastEditTime: 2020-04-08 20:31\"\"\"import scrapyimport refrom myScrapy.myScrapy.items import MyscrapyItemclass mySpider(scrapy.spiders.Spider): # spider的名字 name = \"xuetang\" # 限制spider爬取的域名 allowed_domains = [\"www.xuetangx.com/\"] # 爬虫要爬取的网页，是一个列表，按顺序爬取 start_urls = [\"http://www.xuetangx.com/partners\"] # 这是一种方式，可以爬取网页中所有的项 # def parse(self, response): # item = MyscrapyItem() # for each in response.xpath(\"/html/body/article[1]/section/ul/*\"): # item['school'] = each.xpath(\"a/div[2]/h3/text()\").extract() # item['num'] = each.xpath(\"a/div[2]/p[1]/text()\").extract() # if item['num']: # item['num'] = re.findall(r'\\d+', item['num'][0]) # if item['school'] and item['num']: # yield (item) # 这是第二种方式，使用for循环，制定爬取项的数目 def parse(self, response): item = MyscrapyItem() for i in range(1, 144): item['school'] = response.xpath \\ (\"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/h3/text()\".format(i)).extract() item['num'] = response.xpath \\ (\"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/p[1]/text()\".format(i)).extract() # 判断爬取的项目是否为空，把非空的项目提交 if item['school'] and item['num']: yield (item)③可以开始运行啦在运行前要先在项目根目录下建立一个begin.py文件来控制scrapy爬虫的运行。123from scrapy import cmdline# \"xuetang\"是我们上面spider.py中定义的爬虫名cmdline.execute(\"scrapy crawl xuetang\".split())最终的项目结构（__init__.py没啥用，删了）：运行begin.py就可以开始爬虫了。出现这些提示就表示成功了，运行完毕后会发现项目根目录出现了我们在pipelines.py中设置好的MyData.json。打开MyData.json看一下，Perfect！有了这个json文件，我们就可以利用Python的pandas、numpy等工具进行各种处理，然后用matplotlib等模块进行画图了。三、总结本文只是对Python的scrapy爬虫进行了简单的介绍和用一个小Demo讲述了如何使用scrapy爬取网页数据，希望对你有所帮助。","categories":[{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/categories/Python/"},{"name":"爬虫","slug":"Python/爬虫","permalink":"https://666wxy666.github.io/categories/Python/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://666wxy666.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"数据处理","slug":"数据处理","permalink":"https://666wxy666.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"学堂在线","slug":"学堂在线","permalink":"https://666wxy666.github.io/tags/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF/"},{"name":"课程","slug":"课程","permalink":"https://666wxy666.github.io/tags/%E8%AF%BE%E7%A8%8B/"}]},{"title":"Linux 上机实战1 正则表达式","slug":"Linux-上机实战1-正则表达式","date":"2020-03-31T12:46:39.215Z","updated":"2020-04-29T03:34:57.130Z","comments":true,"path":"2020/03/31/Linux-上机实战1-正则表达式/","link":"","permalink":"https://666wxy666.github.io/2020/03/31/Linux-%E4%B8%8A%E6%9C%BA%E5%AE%9E%E6%88%981-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"本文是关于Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式应用的一个样例，获取北京某时刻PM2.5的数据，然后进行处理，输出到csv文件中，并画图表展示。","text":"本文是关于Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式应用的一个样例，获取北京某时刻PM2.5的数据，然后进行处理，输出到csv文件中，并画图表展示。一、题目要求：​ 从因特网上搜索 Web 页，用 wget 获取网页，处理网页 html 文本数据，从中提取出当前时间点北京各监测站的 PM2.5 浓度，输出如下 CSV 格式数据：2020 03 09 13:00:00, 海淀区万柳 ,732020 03 09 13:00:00, 昌平镇 ,672020 03 09 13:00:00, 奥体中心 ,662020 03 09 13:00:00, 海淀区万柳 ,732020 03 09 13:00:00, 昌平镇 ,732020 03 09 13:00:00, 奥体中心 ,75​ 撰写实验报告，要求：写出对数据的分析和处理思路，列出各个处理步骤并给出解释。二、详细步骤：从因特网上搜索 Web 页，找到与含有北京各监测站的 PM2.5 浓度的网站，我找到了绿色呼吸网（http://www.pm25.com/city/beijing.html），网站如下：使用Xshell登录到Ubuntu服务器： 使用wget命令获取该网页：1wget http://www.pm25.com/city/beijing.html使用cat命令查看该网页的内容：1cat beijing.html | more我们关注的内容：​ ①数据更新的时间：​ ②各监测点PM2.5浓度数据：发现时间的地方有个“更新时间：”，监测点名称的地方都有“pjadt_location”，而PM2.5浓度的地方都有“pjadt_pm25”。根据这个特性，先使用awk命令将需要的行保留下来。下面先进行编写1.awk：1vim 1.awk运行以下命令，对所需行进行过滤：1cat beijing.html | awk -f 1.awk | more发现除了我们想要的行还多出了这几行：经过观察，发现“PM2.5”浓度这一行与我们所需的行的区别是，我们所需的行有μg，而“PM2.5”浓度这一行没有：我们重新对“1.awk”进行编辑，直接将“监测站点”这一行排除，并且对“PM2.5”浓度这一行采用额外的过滤规则:重新运行以下命令，对所需行进行过滤：1cat beijing.html | awk -f 1.awk | more发现已经筛选出了所需要的行。现在再利用sed命令将html标签”&lt;&gt;“中的内容和“更新时间：”这个无用的信息删除：1cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | more发现已经只剩余我们想要的数据。但是这些数据不在同一行，且没有明显特征，无法进行awk命令，因此先使用tr命令将这些行合并为一行，以空格分隔：1cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\\n' ' ' | more最后利用awk命令将所需内容规格化输出即可，先编辑“2.awk”：1vim 2.awk利用for循环输出，NF为列数，刚好循环$$(NF-2)/4$$次，第一个%s为日期，第二个%s为时间，第三个%s为监测点名称，第四个%s为PM2.5浓度，再运行以下awk命令，即可得到格式化的输出：1cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\\n' ' ' | awk -f 2.awk | more发现输出已经符合题目要求。将结果重定向到文件“beijing.csv”：12cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\\n' ' ' | awk -f 2.awk &gt; beijing.csvvim beijing.csv 将beijing.csv发送到电脑，并将编码转为ANSI：由于不同时间的数据有所不同，因此我的过滤语句考虑了不同时间点的情况（北京一共有12个监测点，有时有的监测点没有数据，因此是只有11个监测点的数据），直接运行以下命令就可以直接将数据导出为beijing.csv，以下为另一时间点的数据情况：12wget http://www.pm25.com/city/beijing.htmlcat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\\n' ' ' | awk -f 2.awk &gt; beijing.csv三、总结本文以一个样例，详细介绍了Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式的相关知识，希望对你的Linux学习有所帮助。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"上机实战","slug":"Linux/上机实战","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B8%8A%E6%9C%BA%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"实战","slug":"实战","permalink":"https://666wxy666.github.io/tags/%E5%AE%9E%E6%88%98/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://666wxy666.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"Hello Blog","slug":"我的第一篇博客","date":"2020-03-19T03:08:51.637Z","updated":"2020-03-31T12:21:28.836Z","comments":true,"path":"2020/03/19/我的第一篇博客/","link":"","permalink":"https://666wxy666.github.io/2020/03/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","excerpt":"HELLO BLOG欢迎来到我的博客，这是我的第一篇博客测试文章。","text":"HELLO BLOG欢迎来到我的博客，这是我的第一篇博客测试文章。如果有什么问题，欢迎到我的Github提问我。","categories":[{"name":"其他","slug":"其他","permalink":"https://666wxy666.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"first","slug":"first","permalink":"https://666wxy666.github.io/tags/first/"}]}],"categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"习题","slug":"Linux/习题","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B9%A0%E9%A2%98/"},{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/categories/Python/"},{"name":"爬虫","slug":"Python/爬虫","permalink":"https://666wxy666.github.io/categories/Python/%E7%88%AC%E8%99%AB/"},{"name":"上机实战","slug":"Linux/上机实战","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B8%8A%E6%9C%BA%E5%AE%9E%E6%88%98/"},{"name":"其他","slug":"其他","permalink":"https://666wxy666.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"习题","slug":"习题","permalink":"https://666wxy666.github.io/tags/%E4%B9%A0%E9%A2%98/"},{"name":"易错点","slug":"易错点","permalink":"https://666wxy666.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"网课","slug":"网课","permalink":"https://666wxy666.github.io/tags/%E7%BD%91%E8%AF%BE/"},{"name":"MOOC","slug":"MOOC","permalink":"https://666wxy666.github.io/tags/MOOC/"},{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://666wxy666.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"数据处理","slug":"数据处理","permalink":"https://666wxy666.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"学堂在线","slug":"学堂在线","permalink":"https://666wxy666.github.io/tags/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF/"},{"name":"课程","slug":"课程","permalink":"https://666wxy666.github.io/tags/%E8%AF%BE%E7%A8%8B/"},{"name":"实战","slug":"实战","permalink":"https://666wxy666.github.io/tags/%E5%AE%9E%E6%88%98/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://666wxy666.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"first","slug":"first","permalink":"https://666wxy666.github.io/tags/first/"}]}