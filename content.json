{"meta":{"title":"苇名一心的博客","subtitle":null,"description":"经验分享和技术交流。","author":"苇名一心","url":"https://666wxy666.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-04-01T09:45:17.617Z","updated":"2020-03-25T08:57:28.904Z","comments":true,"path":"404.html","permalink":"https://666wxy666.github.io/404.html","excerpt":"","text":"404很抱歉，您访问的页面不存在可能是输入地址有误或该地址已被删除，您可以留言告诉我您要访问哪个页面找不到了，谢谢。"},{"title":"所有分类","date":"2020-04-01T09:45:17.618Z","updated":"2020-03-22T10:40:41.483Z","comments":true,"path":"categories/index.html","permalink":"https://666wxy666.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-04-01T09:45:17.618Z","updated":"2020-03-25T07:29:48.422Z","comments":true,"path":"faqs/index.html","permalink":"https://666wxy666.github.io/faqs/index.html","excerpt":"","text":"README"},{"title":"所有标签","date":"2020-04-01T09:45:17.619Z","updated":"2020-03-25T07:51:09.297Z","comments":true,"path":"tags/index.html","permalink":"https://666wxy666.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2020-04-01T09:45:17.618Z","updated":"2020-03-22T01:11:48.329Z","comments":true,"path":"mylist/index.html","permalink":"https://666wxy666.github.io/mylist/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2020-04-07T12:01:17.904Z","updated":"2020-04-07T12:01:17.904Z","comments":true,"path":"friends/index.html","permalink":"https://666wxy666.github.io/friends/index.html","excerpt":"这里是我的朋友和一些大佬，感兴趣的就点进去看看吧！","text":"这里是我的朋友和一些大佬，感兴趣的就点进去看看吧！想在这里展示或者有什么问题欢迎给我留言。"},{"title":"关于","date":"2020-04-27T12:38:07.457Z","updated":"2020-04-27T12:38:07.457Z","comments":true,"path":"about/index.html","permalink":"https://666wxy666.github.io/about/index.html","excerpt":"","text":"这里是我的基本信息：我的博客（Github）：苇名一心我的博客（Gitee）：苇名一心我的Github主页：666WXY666我的Gitee主页：wxy_666我的B站：在下-苇名一心我的知乎：苇名一心我的网易云：在下-苇名一心我的微博：在下-苇名一心欢迎关注我，与我联系。"}],"posts":[{"title":"Linux 拓展学习","slug":"Linux-拓展学习","date":"2020-04-29T12:40:47.779Z","updated":"2020-04-29T13:59:57.423Z","comments":true,"path":"2020/04/29/Linux-拓展学习/","link":"","permalink":"https://666wxy666.github.io/2020/04/29/Linux-%E6%8B%93%E5%B1%95%E5%AD%A6%E4%B9%A0/","excerpt":"简单的写了一些Linux的课外拓展学习的相关知识和问题","text":"简单的写了一些Linux的课外拓展学习的相关知识和问题一、命令的参数编写一个小程序，可以显示命令的选项和参数1234567891011#include &lt;stdio.h&gt;int main(int argc, char const *argv[])&#123; int i; for (i = 0; i &lt; argc; i++) &#123; printf(\"%d:%p [%s]\\n\", i, argv[i], argv[i]); &#125; return 0;&#125;效果：二、关于控制printf输出的颜色编写程序hello.c，编译和运行程序，得到类似以下结果：可以通过1\\033[来控制printf输出的颜色，格式为：1printf(\"\\033[字背景颜色;字体颜色m 字符串 \\033[0m\" );注意，一定要在printf最后使用1\\033[0m来消除前面的作用，否则前面的设置对后面的printf都有效果。ANSI控制码效果\\033[0m关闭所有属性\\033[1m设置高亮度\\033[4m下划线\\033[5m闪烁\\033[7m反显\\033[8m消隐\\033[30m —\\033[37m设置前景色\\033[40m—\\033[47m设置背景色\\033[nA光标上移n行\\03[nB光标下移n行\\033[nC光标右移n行\\033[nD光标左移n行字背景颜色范围: 40–49字颜色范围: 30—39字背景颜色代号字背景颜色字颜色代号字颜色40黑30黑41红31红42绿32绿43黄33黄44蓝34蓝45紫35紫46青36青47白37白小程序具体代码：12345678910#define NONE \"\\e[0m\"#define RED \"\\e[0;31m\"#define CYAN \"\\e[0;36m\"#include &lt;stdio.h&gt;int main(int argc, char const *argv[])&#123; printf(RED \"Hello \" CYAN \"World!\\n\" NONE); printf(\"\\e[0;31mHello \\e[0;36mWorld!\\n\\e[0m\"); return 0;&#125;在这里\\e和\\033是一个意思，e的八进制代码就是033，这两个print其实效果是一样的，前面那个printf中多个连续的“xxx”字符串会被自动整合为一个字符串，其实最终执行的就是第二个printf。效果：三、关于Linux中bash的变量、替换和元字符要求只列出所有bash进程的状态，使用命令1ps -ef | grep bash但grep进程自身也被输出了：使用以下的命令：12345ps -ef | grep [b]ashps -ef | grep \\\\bashps -ef | grep b\\\\ashps -ef | grep b\\ashps -ef | grep ba\\\\sh可以上面这5个命令的核心目的其实是改变grep在1ps -ef命令中的显示形式，例如这一个，我们使用三通（tee）命令将在grep命令过滤之前的输出定向到一个文件中。先将原命令试一下：12ps -ef | tee 1.txt | grep bashcat 1.txt与1这个命令进行一下对比：12ps -ef | tee 1.txt | grep [b]ashcat 1.txt我们可以很明显的看到，grep命令确实改变了，因为正则表达式1[b]ash是可以匹配bash的，但是却不能匹配[b]ash，因此第一行的-bash这一行被取了出来，而最后grep这一行，由于不匹配[b]ash，就取不出来了，刚好实现了我们只列出所有bash进程的状态的目的，下面的几个命令大同小异。不可以12ps -ef | tee 1.txt | grep \\\\bashcat 1.txt首先，\\是Shell的元字符，先被\\转义为真正的单个字符\\，因此传给grep的正则表达式其实是\\bash，而在正则表达式中，\\b有特殊含义，是单词边界，因此就连bash和\\bash全部都没有匹配到。可以12ps -ef | tee 1.txt | grep b\\\\ashcat 1.txt和2一样，传给grep的正则表达式为b\\ash，在正则表达式中\\a是没有意义的，因此\\被丢弃，正则表达式其实就是bash，所以第一行的-bash这一行被取了出来，而最后grep这一行，由于不匹配b\\ash，就取不出来了。不可以12ps -ef | tee 1.txt | grep b\\ashcat 1.txt首先，\\a先经过Shell的替换，因\\a是没有特殊含义的，因此\\被直接丢掉，传给grep的正则表达式为bash，这就和使用这个命令是一样的：1ps -ef | grep bash显然是不能实现目的。不可以12ps -ef | tee 1.txt | grep ba\\\\shcat 1.txt这个和2基本是一样的，和3唯一的区别就是，第3个中\\a对正则表达式而言是没有特殊含义的，但是\\s对正则表达式而言是有特殊含义的，\\s代表空白字符（可能是空格、制表符、其他空白），也就是说，grep匹配到的应该是类似于：1ba h这种的字符串，显然-bash和ba\\sh都不符合，因此就什么都没取到。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"拓展学习","slug":"Linux/拓展学习","permalink":"https://666wxy666.github.io/categories/Linux/%E6%8B%93%E5%B1%95%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"拓展","slug":"拓展","permalink":"https://666wxy666.github.io/tags/%E6%8B%93%E5%B1%95/"},{"name":"难点","slug":"难点","permalink":"https://666wxy666.github.io/tags/%E9%9A%BE%E7%82%B9/"}]},{"title":"Linux MOOC习题 5~9章","slug":"Linux-MOOC习题-5~9章","date":"2020-04-29T01:54:47.814Z","updated":"2020-04-29T05:28:08.649Z","comments":true,"path":"2020/04/29/Linux-MOOC习题-5~9章/","link":"","permalink":"https://666wxy666.github.io/2020/04/29/Linux-MOOC%E4%B9%A0%E9%A2%98-5~9%E7%AB%A0/","excerpt":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的，接上文Linux MOOC习题 1~4章。","text":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的，接上文Linux MOOC习题 1~4章。PS：第七章是上机实验，第一次上机实验详见我的另一篇文章：Linux 上机实战1 正则表达式，第二次的实验因为是提交阶段暂不公开，后面可能会发。五、文件管理和目录管理 在Linux中，Shell会自动帮你进项一些替换，实际运行的命令其实是：1cp src/x.c src/xx.c src/xxx.c src/x.h src/xx.h src/xxx.h……如果有好多.c和.h文件（超过两个），cp就会报错：cp: target xxx is not a directory但是如果只有两个.c或.h文件，那么就会产生非常严重的后果，实际运行了下面的命令：1cp src/x.c src/y.c这是非常危险的，这样y.c的内容会被x.c覆盖，这是我们不愿意看到的。那么应该用什么命令来实现题目的要求呢？1cp src/*.[ch] .我们显示地指定复制到.（当前目录），就不会出现问题了。这也是在Linux中很怪的地方，明明你不能写这个文件，但是你却可以删除，如果不加-f，删除只读文件时会有提示，但是加了-f就没有任何提示了。还有一点需要注意的是，如第6题所说，加了-f也不能删除无权限删除的文件。 关于第10题，不像Windows，在Linux中这些后缀都是约定俗成的，只是为了做标记用，好区分而已，没有实际意义。六、Linux命令风格和文件系统这两个题都忽略了“符号链接”的作用。 这俩没啥好说的。在Linux中，一般是-1代表失败，&gt;=0的值代表成功。八、文件和目录的权限、Shell目录有执行权限（即x权限）意味着分析路径名过程中可检索该目录。九、替换、元字符和转义如果你是执行这个命令的操作员，估计你马上就心情不好了。由于拼错了单词，把DATABASE不小心写成了DATEBASE，灾难来了。未命名的变量被bash替换为空字符串，实际上你以root身份执行了最邪恶的一条命令：1rm -rf /*永远不要盲目自信，谁也保证不了自己不会犯错误，怎么才能避免这样的悲剧发生？尽量不要以root身份登录。bash有选项，引用未定义的变量会出错而不是替换为空字符串，可以打开这个选项。早期的编程语言把引用的未定义过的变量自动加上定义，这种做法实际上太糟糕，最早的FORTRAN语言就这种做法，据说曾因此导致一次太空任务失败。在设计你自己的系统时，建库命令能够从目录名开始建库，就是说要求建库之前/opt/data下不需要存在目录puma，而不是要求建库之前/opt/data有个空目录puma，这样的话，即使你的命令变成了1rm -rf $DATEBASE因为你的失误，会导致rm命令抱怨缺少参数而什么都不做，这个结果是可以接受的。在Linux中，目前我已知的Shell的元字符有：1空格 制表符 回车 &gt; &lt; | ; &amp; $ * [ ] ? \\ ( ) '' \" \" 反撇号`第11题中，这三个命令都可以取消文件通配符*的特殊作用，让echo直接打印字符*，并且这个替换是Shell进行的，也就是说，echo拿到的命令都是一样的，他是分不清操作员输入的是哪个命令，这三个命令的参数：而对于不转义的命令的参数：1echo *因此结果就很明显了：未完待续……所有的习题：Linux MOOC习题 1~4章Linux MOOC习题 5~9章","categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"习题","slug":"Linux/习题","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B9%A0%E9%A2%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"习题","slug":"习题","permalink":"https://666wxy666.github.io/tags/%E4%B9%A0%E9%A2%98/"},{"name":"易错点","slug":"易错点","permalink":"https://666wxy666.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"网课","slug":"网课","permalink":"https://666wxy666.github.io/tags/%E7%BD%91%E8%AF%BE/"},{"name":"MOOC","slug":"MOOC","permalink":"https://666wxy666.github.io/tags/MOOC/"}]},{"title":"Linux MOOC习题 1~4章","slug":"Linux-MOOC习题-1~4章","date":"2020-04-28T13:08:08.946Z","updated":"2020-04-29T05:24:53.709Z","comments":true,"path":"2020/04/28/Linux-MOOC习题-1~4章/","link":"","permalink":"https://666wxy666.github.io/2020/04/28/Linux-MOOC%E4%B9%A0%E9%A2%98-1~4%E7%AB%A0/","excerpt":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的","text":"自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的废话不多说，直接开始：PS：第一章是课程介绍。二、开始使用Linux和文本文件的处理第1题其实是很基础的问题，早期的终端一般由键盘、显示器和RS232串行通信接口构成，没有磁盘存储器，其实可以想象成和打字机差不多的东西，他就是直接在显示器上显示了，不需要磁盘存储器。第2题也比较基础：行律的作用是：一行内字符的缓冲、回显和编辑，直到按下回车键；数据的加工，类似第二题中的将“\\n”替换为“\\r\\n”；将CTRL-C字符转换为终止进程的信号；驱动程序其实是串口与行律的接口，负责上行和下行字符流。这俩题没啥好说的，基础知识。并不是这样的，CTRL-C确实传送了字符（Ctrl+字母组合键可以产生ASCII码为1-26的控制字符，字母序号是几，ASCII码就是几，Ctrl+C的ASCII码应为3），其实还是行律的作用，他将CTRL-C字符转换为终止进程的信号，从而通知Linux主机，进程终止。 这俩题都是关于uniq命令的。123uniq -uuniq -duniq第一个命令，-u代表unique，只打印没有重复的行；第二个命令，-d代表duplicated，只打印重复的行，注意重复的行只打印一次；第三个命令，啥也不加就是都打印，但是也是复的行只打印一次；还有就是关于重复的行，意思其实是连续的紧邻的两行内容相同才被叫做重复的行，因此第10题是错的。这个题不难，写出来的目的是只要记得Less is more就很好想了，less是more的升级版。 这俩题也不多解释了，od不可打印字符也可以显示，tr是用于翻译，把string1出现的字符替换为string2中对应的字符，ASCII字符0也可以翻译。1tr string1 string2三、正则表达式先说一下正则表达式的元字符，有6个，分别是：1. * [ \\ ^ $关于他们各自的含义不多说，自行百度，一定要注意的是]不是元字符，很容易搞错。而关于单字符正则表达式，显而易见，就是匹配一个字符呗，不过有特殊的:\\加一个字符构成的转义字符，看做单字符正则表达式；1\\. \\* \\[ \\\\ \\^ \\$[]定义的集合也被看做单字符正则表达式;因为.本来的含义是匹配任意字符，转义后就是单字符.，因此都是单字符正则表达式，而$在尾部时和^在首部时有特殊含义，因此转义之前不是，转义后就是单字符​了。这个题就比较有意思了，先看一下sed命令的基本用法：1sed 's/正则表达式/字符串/g' 文件名列表s选项的意思是替换，将文件中匹配第一个//中的正则表达式的内容替换为第二个//中的字符串。然后我们在看上题‘’中的正则表达式（也就是第一个//中的内容）1\\[[^][]*]可以分为三部分：因为[是元字符，加\\转义后就变成了真正的[。1\\[中间这一部分是一个集合[]，重复一次或多次[]*，集合里面的^代表排除，排除了]和[，意思就是说除]和[外的字符重复一次或多次。那么这就有一个问题，第一个[为什么不与第一个]匹配，反而去和最后一个]匹配呢？个人觉得，因为如果与第一个]匹配，就有了[^]，这显然是错误的正则表达式，那么就继续向后匹配，因此就匹配到了最后面的]。1[^][]*因为]不是元字符，也没有[与它匹配，它就是单纯的一个字符]。1]综上所述，这个正则表达式可以匹配“[非]和[的字符任意多个]”，类似[参考文献23]这样的，但是如果[]里面还有]和[，就匹配不到了。再看上面的题，第二个//里面是空的，那么就是把匹配到的内容删除，很好理解。四、文件比较，文件通配符vi的基本用法，在命令状态12345678:q 退出（:quit的缩写）:q! 退出且不保存（:quit!的缩写）:wq 保存并退出:wq! 保存并退出即使文件没有写入权限（强制保存退出）:x 保存并退出（类似:wq，但是只有在有更改的情况下才保存）:exit 保存并退出（和:x相同）:qa 退出所有(:quitall的缩写):cq 退出且不保存（即便有错误）另外在“正常模式”下输入“ZZ”来保存并退出Vim（和:x相同），或者“ZQ”不保存并退出（和:q!相同）注意此处ZZ大写和小写是完全不同的。diff一般用于比较文本文件。未完待续……所有的习题：Linux MOOC习题 1~4章Linux MOOC习题 5~9章","categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"习题","slug":"Linux/习题","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B9%A0%E9%A2%98/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"习题","slug":"习题","permalink":"https://666wxy666.github.io/tags/%E4%B9%A0%E9%A2%98/"},{"name":"易错点","slug":"易错点","permalink":"https://666wxy666.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"网课","slug":"网课","permalink":"https://666wxy666.github.io/tags/%E7%BD%91%E8%AF%BE/"},{"name":"MOOC","slug":"MOOC","permalink":"https://666wxy666.github.io/tags/MOOC/"}]},{"title":"Python爬虫小Demo：学堂在线课程","slug":"Python爬虫小Demo","date":"2020-04-07T11:43:33.300Z","updated":"2020-04-08T14:42:43.756Z","comments":true,"path":"2020/04/07/Python爬虫小Demo/","link":"","permalink":"https://666wxy666.github.io/2020/04/07/Python%E7%88%AC%E8%99%AB%E5%B0%8FDemo/","excerpt":"本文是关于Python中的Spider的小Demo，通过Python的scrapy爬取京学堂在线课程的相关数据。","text":"本文是关于Python中的Spider的小Demo，通过Python的scrapy爬取京学堂在线课程的相关数据。一、编译环境：PyCharm 2019.3.4 (Professional Edition)Build #PY-193.6911.25, built on March 18, 2020Runtime version: 11.0.6+8-b520.43 amd64VM: OpenJDK 64-Bit Server VM by JetBrains s.r.oWindows 10 10.0GC: ParNew, ConcurrentMarkSweepMemory: 725MCores: 8Registry: ide.balloon.shadow.size=0Non-Bundled Plugins:GrepConsole,Statistic,cn.yiiguxing.plugin.translate,com.chrisrm.idea.MaterialThemeUI,com.notime.intellijPlugin.backgroundImagePlus,com.wakatime.intellij.plugin,izhangzhihao.rainbow.brackets,mobi.hsz.idea.gitignore, net.vektah.codeglance, org.intellij.giteePython Version：3.7（Anaconda3）Package：scrapy==2.0.1二、详细步骤①准备工作在Pycharm中新建一个Pure Python项目（记得要按照一中的要求配好Python环境）。打开Pycharm的下方的终端（Terminal），当然这些也可以在系统终端里操作，不过可能需要的步骤多一些，还是直接在Pycharm里方便一些。在终端里输入1scrapy startproject myScrapy本来是可以在我们刚刚创建的项目里新建一个名为“myScrapy”的scrapy项目的，但是不知道为什么竟然报错了：这是什么奇奇怪怪的错误，我都没有“d:\\bld\\scrapy_1584555997548_h_env\\python.exe”这个目录，经过查找相关问题的资料，问题可能是出在Python环境上，因为我上一次新建项目时并没有报错，这一次我直接用的上一个项目的环境，网上还有一种说法是Scrapy的bug，详见Fatal error launching scrapy&gt;1.6.0 from Anaconda PromptIssue with conda-forge scrapy&gt;1.6.0 on Windows目前找到了两种解决方案：把python环境复制到报错的那个目录（d:\\bld\\scrapy_1584555997548_h_env\\python.exe），然后在创建scrapy项目，但这个解决方法有点愚蠢，就没有采用。在scrapy命令前面添加“python -m”选项：1python -m scrapy startproject myScrapy就可以正常创建了。关于Python的-m选项，官方给出的解释是“run library module as a script”，简单来说就是将库中的Python模块当作脚本去运行。特别感谢简书的大佬ccw1078提供的解释，很清晰明了，有兴趣的可以去瞅一下，因为和本文的主题爬虫没啥关系，在这里就不赘述了。出现这些提示就代表创建成功了。然后在spiders文件里新建一个spider.py文件，用于写爬虫。我们来看一下目前scrapy项目的目录结构。__init__.py：pycharm生成的文件，简化导入语句用的，可以忽略，没啥用，建议删了，留着可能会出问题。spiders：存放你Spider爬虫源文件​ spider.py：代码主要在这里写。items.py：数据容器。middlewares.py：Downloader Middlewares(下载器中间件)和Spider Middlewares(蜘蛛中间件)实现的地方。pipelines.py：项目管道文件，相当于数据中转站。实现数据的清洗，储存，验证。settings.py：scrapy的全局配置。scrapy.cfg：配置文件。scrapy已经帮我们把大体框架写好了，我们主要要修改的文件是spider.py，items.py，pipelines.py，settings.py。这是爬虫spider的基本工作方式，想要深入了解的可以去网上查找资料。②开始写代码先来写items.py。123456789import scrapyclass MyscrapyItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() school = scrapy.Field() num = scrapy.Field() pass再来搞settings.py，这个只需要找到这个注释掉的语句，把#去掉就OK了，就像这样：pipelines.py就很好写了，基本可以当模板来用。12345678910111213141516171819import jsonclass MyscrapyPipeline(object): def open_spider(self, spider): try: # 这个就是爬虫生成的文件，可以支持好多种格式，这里使用的是json文件 self.file = open('MyData.json', 'w', encoding=\"utf-8\") except Exception as err: print(err) def process_item(self, item, spider): dict_item = dict(item) json_str = json.dumps(dict_item, ensure_ascii=False) + \"\\n\" self.file.write(json_str) return item def close_spider(self, spider): self.file.close()spider.py是我们主要写的核心部分。这里需要一些html的xpath相关知识来对项进行定位，可以自行查找相关资料。12345678910111213141516171819202122232425262728293031323334353637383940414243\"\"\"@Copyright: Copyright (c) 2020 苇名一心 All Rights Reserved.@Project: xuetangzaixian@Description: @Version: @Author: 苇名一心@Date: 2020-04-08 20:31@LastEditors: 苇名一心@LastEditTime: 2020-04-08 20:31\"\"\"import scrapyimport refrom myScrapy.myScrapy.items import MyscrapyItemclass mySpider(scrapy.spiders.Spider): # spider的名字 name = \"xuetang\" # 限制spider爬取的域名 allowed_domains = [\"www.xuetangx.com/\"] # 爬虫要爬取的网页，是一个列表，按顺序爬取 start_urls = [\"http://www.xuetangx.com/partners\"] # 这是一种方式，可以爬取网页中所有的项 # def parse(self, response): # item = MyscrapyItem() # for each in response.xpath(\"/html/body/article[1]/section/ul/*\"): # item['school'] = each.xpath(\"a/div[2]/h3/text()\").extract() # item['num'] = each.xpath(\"a/div[2]/p[1]/text()\").extract() # if item['num']: # item['num'] = re.findall(r'\\d+', item['num'][0]) # if item['school'] and item['num']: # yield (item) # 这是第二种方式，使用for循环，制定爬取项的数目 def parse(self, response): item = MyscrapyItem() for i in range(1, 144): item['school'] = response.xpath \\ (\"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/h3/text()\".format(i)).extract() item['num'] = response.xpath \\ (\"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/p[1]/text()\".format(i)).extract() # 判断爬取的项目是否为空，把非空的项目提交 if item['school'] and item['num']: yield (item)③可以开始运行啦在运行前要先在项目根目录下建立一个begin.py文件来控制scrapy爬虫的运行。123from scrapy import cmdline# \"xuetang\"是我们上面spider.py中定义的爬虫名cmdline.execute(\"scrapy crawl xuetang\".split())最终的项目结构（__init__.py没啥用，删了）：运行begin.py就可以开始爬虫了。出现这些提示就表示成功了，运行完毕后会发现项目根目录出现了我们在pipelines.py中设置好的MyData.json。打开MyData.json看一下，Perfect！有了这个json文件，我们就可以利用Python的pandas、numpy等工具进行各种处理，然后用matplotlib等模块进行画图了。三、总结本文只是对Python的scrapy爬虫进行了简单的介绍和用一个小Demo讲述了如何使用scrapy爬取网页数据，希望对你有所帮助。","categories":[{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/categories/Python/"},{"name":"爬虫","slug":"Python/爬虫","permalink":"https://666wxy666.github.io/categories/Python/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://666wxy666.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"数据处理","slug":"数据处理","permalink":"https://666wxy666.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"学堂在线","slug":"学堂在线","permalink":"https://666wxy666.github.io/tags/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF/"},{"name":"课程","slug":"课程","permalink":"https://666wxy666.github.io/tags/%E8%AF%BE%E7%A8%8B/"}]},{"title":"Hello Blog","slug":"我的第一篇博客","date":"2020-03-19T03:08:51.637Z","updated":"2020-03-31T12:21:28.836Z","comments":true,"path":"2020/03/19/我的第一篇博客/","link":"","permalink":"https://666wxy666.github.io/2020/03/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","excerpt":"HELLO BLOG欢迎来到我的博客，这是我的第一篇博客测试文章。","text":"HELLO BLOG欢迎来到我的博客，这是我的第一篇博客测试文章。如果有什么问题，欢迎到我的Github提问我。","categories":[{"name":"其他","slug":"其他","permalink":"https://666wxy666.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"first","slug":"first","permalink":"https://666wxy666.github.io/tags/first/"}]}],"categories":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/categories/Linux/"},{"name":"拓展学习","slug":"Linux/拓展学习","permalink":"https://666wxy666.github.io/categories/Linux/%E6%8B%93%E5%B1%95%E5%AD%A6%E4%B9%A0/"},{"name":"习题","slug":"Linux/习题","permalink":"https://666wxy666.github.io/categories/Linux/%E4%B9%A0%E9%A2%98/"},{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/categories/Python/"},{"name":"爬虫","slug":"Python/爬虫","permalink":"https://666wxy666.github.io/categories/Python/%E7%88%AC%E8%99%AB/"},{"name":"其他","slug":"其他","permalink":"https://666wxy666.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://666wxy666.github.io/tags/Linux/"},{"name":"拓展","slug":"拓展","permalink":"https://666wxy666.github.io/tags/%E6%8B%93%E5%B1%95/"},{"name":"难点","slug":"难点","permalink":"https://666wxy666.github.io/tags/%E9%9A%BE%E7%82%B9/"},{"name":"习题","slug":"习题","permalink":"https://666wxy666.github.io/tags/%E4%B9%A0%E9%A2%98/"},{"name":"易错点","slug":"易错点","permalink":"https://666wxy666.github.io/tags/%E6%98%93%E9%94%99%E7%82%B9/"},{"name":"网课","slug":"网课","permalink":"https://666wxy666.github.io/tags/%E7%BD%91%E8%AF%BE/"},{"name":"MOOC","slug":"MOOC","permalink":"https://666wxy666.github.io/tags/MOOC/"},{"name":"Python","slug":"Python","permalink":"https://666wxy666.github.io/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://666wxy666.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"数据处理","slug":"数据处理","permalink":"https://666wxy666.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"学堂在线","slug":"学堂在线","permalink":"https://666wxy666.github.io/tags/%E5%AD%A6%E5%A0%82%E5%9C%A8%E7%BA%BF/"},{"name":"课程","slug":"课程","permalink":"https://666wxy666.github.io/tags/%E8%AF%BE%E7%A8%8B/"},{"name":"first","slug":"first","permalink":"https://666wxy666.github.io/tags/first/"}]}