<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python爬虫小Demo：学堂在线课程</title>
      <link href="/2020/04/07/Python%E7%88%AC%E8%99%AB%E5%B0%8FDemo/"/>
      <url>/2020/04/07/Python%E7%88%AC%E8%99%AB%E5%B0%8FDemo/</url>
      
        <content type="html"><![CDATA[<p><strong>本文是关于Python中的Spider的小Demo，通过Python的scrapy爬取京学堂在线课程的相关数据。</strong></p><a id="more"></a><h2 id="一、编译环境："><a href="#一、编译环境：" class="headerlink" title="一、编译环境："></a>一、编译环境：</h2><p>PyCharm 2019.3.4 (Professional Edition)</p><p>Build #PY-193.6911.25, built on March 18, 2020</p><p>Runtime version: 11.0.6+8-b520.43 amd64</p><p>VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o</p><p>Windows 10 10.0</p><p>GC: ParNew, ConcurrentMarkSweep</p><p>Memory: 725M</p><p>Cores: 8</p><p>Registry: ide.balloon.shadow.size=0</p><p>Non-Bundled Plugins:</p><p>GrepConsole,Statistic,cn.yiiguxing.plugin.translate,com.chrisrm.idea.MaterialThemeUI,com.notime.intellijPlugin.backgroundImagePlus,com.wakatime.intellij.plugin,izhangzhihao.rainbow.brackets,mobi.hsz.idea.gitignore, net.vektah.codeglance, org.intellij.gitee</p><p>Python Version：3.7（Anaconda3）</p><p>Package：</p><p>scrapy==2.0.1</p><h2 id="二、详细步骤"><a href="#二、详细步骤" class="headerlink" title="二、详细步骤"></a>二、详细步骤</h2><h3 id="①准备工作"><a href="#①准备工作" class="headerlink" title="①准备工作"></a>①准备工作</h3><ol><li>在Pycharm中新建一个Pure Python项目（记得要按照一中的要求配好Python环境）。</li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200407214757.jpg" alt="2020-04-07_214715" style="zoom:67%"><ol start="2"><li>打开Pycharm的下方的终端（Terminal），当然这些也可以在系统终端里操作，不过可能需要的步骤多一些，还是直接在Pycharm里方便一些。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408164806.png" alt="2020-04-08_164744"></p><ol start="3"><li><p>在终端里输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myScrapy</span><br></pre></td></tr></table></figure><p>本来是可以在我们刚刚创建的项目里新建一个名为“myScrapy”的scrapy项目的，但是不知道为什么竟然报错了：</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408190345.png" alt="2020-04-08_190330"></p><p>这是什么奇奇怪怪的错误，我都没有“d:\bld\scrapy_1584555997548_h_env\python.exe”这个目录，经过查找相关问题的资料，问题可能是出在Python环境上，因为我上一次新建项目时并没有报错，这一次我直接用的上一个项目的环境，网上还有一种说法是Scrapy的bug，详见</p><p><a href="https://github.com/scrapy/scrapy/issues/4289" target="_blank" rel="external nofollow noopener noreferrer">Fatal error launching scrapy&gt;1.6.0 from Anaconda Prompt</a></p><p><a href="https://github.com/conda-forge/scrapy-feedstock/issues/37" target="_blank" rel="external nofollow noopener noreferrer">Issue with conda-forge scrapy&gt;1.6.0 on Windows</a></p><p>目前找到了两种解决方案：</p><ol><li><p>把python环境复制到报错的那个目录（d:\bld\scrapy_1584555997548_h_env\python.exe），然后在创建scrapy项目，但这个解决方法有点愚蠢，就没有采用。</p></li><li><p>在scrapy命令前面添加“python -m”选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m scrapy startproject myScrapy</span><br></pre></td></tr></table></figure><p>就可以正常创建了。</p><p>关于Python的-m选项，官方给出的解释是“run library module as a script”，简单来说就是将库中的Python模块当作脚本去运行。</p><p>特别感谢简书的大佬<a href="https://www.jianshu.com/p/323fc9a1d7d2" target="_blank" rel="external nofollow noopener noreferrer">ccw1078</a>提供的解释，很清晰明了，有兴趣的可以去瞅一下，因为和本文的主题爬虫没啥关系，在这里就不赘述了。</p></li></ol></li></ol><ol start="4"><li><p>出现这些提示就代表创建成功了。</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408192211.jpg" alt="2020-04-08_192151"></p><p>然后在spiders文件里新建一个spider.py文件，用于写爬虫。</p></li><li><p>我们来看一下目前scrapy项目的目录结构。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408203358.jpg" alt="2020-04-08_203310" style="zoom:67%"><p>__init__.py：pycharm生成的文件，简化导入语句用的，可以忽略，没啥用，建议删了，留着可能会出问题。</p><p>spiders：存放你Spider爬虫源文件</p><p>​ spider.py：代码主要在这里写。</p><p>items.py：数据容器。</p><p>middlewares.py：Downloader Middlewares(下载器中间件)和Spider Middlewares(蜘蛛中间件)实现的地方。</p><p>pipelines.py：项目管道文件，相当于数据中转站。实现数据的清洗，储存，验证。</p><p>settings.py：scrapy的全局配置。</p><p>scrapy.cfg：配置文件。</p><p>scrapy已经帮我们把大体框架写好了，我们主要要修改的文件是spider.py，items.py，pipelines.py，settings.py。</p></li><li><p>这是爬虫spider的基本工作方式，想要深入了解的可以去网上查找资料。</p></li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408201638.png" alt="scrapy" style="zoom:33%"><h3 id="②开始写代码"><a href="#②开始写代码" class="headerlink" title="②开始写代码"></a>②开始写代码</h3><ol><li><p>先来写items.py。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyscrapyItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    school = scrapy.Field()</span><br><span class="line">    num = scrapy.Field()</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>再来搞settings.py，这个只需要找到这个注释掉的语句，把#去掉就OK了，就像这样：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408210722.jpg" alt="2020-04-08_210710" style="zoom:67%"></li><li><p>pipelines.py就很好写了，基本可以当模板来用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyscrapyPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 这个就是爬虫生成的文件，可以支持好多种格式，这里使用的是json文件</span></span><br><span class="line">            self.file = open(<span class="string">'MyData.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> err:</span><br><span class="line">            print(err)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        dict_item = dict(item)</span><br><span class="line">        json_str = json.dumps(dict_item, ensure_ascii=<span class="literal">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(json_str)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure></li><li><p>spider.py是我们主要写的核心部分。这里需要一些html的xpath相关知识来对项进行定位，可以自行查找相关资料。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">@Copyright: Copyright (c) 2020 苇名一心 All Rights Reserved.</span></span><br><span class="line"><span class="string">@Project: xuetangzaixian</span></span><br><span class="line"><span class="string">@Description: </span></span><br><span class="line"><span class="string">@Version: </span></span><br><span class="line"><span class="string">@Author: 苇名一心</span></span><br><span class="line"><span class="string">@Date: 2020-04-08 20:31</span></span><br><span class="line"><span class="string">@LastEditors: 苇名一心</span></span><br><span class="line"><span class="string">@LastEditTime: 2020-04-08 20:31</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> myScrapy.myScrapy.items <span class="keyword">import</span> MyscrapyItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mySpider</span><span class="params">(scrapy.spiders.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># spider的名字</span></span><br><span class="line">    name = <span class="string">"xuetang"</span></span><br><span class="line">    <span class="comment"># 限制spider爬取的域名</span></span><br><span class="line">    allowed_domains = [<span class="string">"www.xuetangx.com/"</span>]</span><br><span class="line"><span class="comment"># 爬虫要爬取的网页，是一个列表，按顺序爬取</span></span><br><span class="line">    start_urls = [<span class="string">"http://www.xuetangx.com/partners"</span>]</span><br><span class="line"><span class="comment"># 这是一种方式，可以爬取网页中所有的项</span></span><br><span class="line">    <span class="comment"># def parse(self, response):</span></span><br><span class="line">    <span class="comment">#     item = MyscrapyItem()</span></span><br><span class="line">    <span class="comment">#     for each in response.xpath("/html/body/article[1]/section/ul/*"):</span></span><br><span class="line">    <span class="comment">#         item['school'] = each.xpath("a/div[2]/h3/text()").extract()</span></span><br><span class="line">    <span class="comment">#         item['num'] = each.xpath("a/div[2]/p[1]/text()").extract()</span></span><br><span class="line">    <span class="comment">#         if item['num']:</span></span><br><span class="line">    <span class="comment">#             item['num'] = re.findall(r'\d+', item['num'][0])</span></span><br><span class="line">    <span class="comment">#         if item['school'] and item['num']:</span></span><br><span class="line">    <span class="comment">#             yield (item)</span></span><br><span class="line">    <span class="comment"># 这是第二种方式，使用for循环，制定爬取项的数目</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        item = MyscrapyItem()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">144</span>):</span><br><span class="line">            item[<span class="string">'school'</span>] = response.xpath \</span><br><span class="line">                (<span class="string">"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/h3/text()"</span>.format(i)).extract()</span><br><span class="line">            item[<span class="string">'num'</span>] = response.xpath \</span><br><span class="line">                (<span class="string">"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/p[1]/text()"</span>.format(i)).extract()</span><br><span class="line">            <span class="comment"># 判断爬取的项目是否为空，把非空的项目提交</span></span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'school'</span>] <span class="keyword">and</span> item[<span class="string">'num'</span>]:</span><br><span class="line">                <span class="keyword">yield</span> (item)</span><br></pre></td></tr></table></figure></li></ol><h3 id="③可以开始运行啦"><a href="#③可以开始运行啦" class="headerlink" title="③可以开始运行啦"></a>③可以开始运行啦</h3><ol><li><p>在运行前要先在项目根目录下建立一个begin.py文件来控制scrapy爬虫的运行。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408212615.jpg" alt="2020-04-08_212604" style="zoom:80%"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"><span class="comment"># "xuetang"是我们上面spider.py中定义的爬虫名</span></span><br><span class="line">cmdline.execute(<span class="string">"scrapy crawl xuetang"</span>.split())</span><br></pre></td></tr></table></figure></li><li><p>最终的项目结构（__init__.py没啥用，删了）：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408212844.jpg" alt="2020-04-08_212834" style="zoom:67%"></li><li><p>运行begin.py就可以开始爬虫了。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408212955.jpg" alt="2020-04-08_212946" style="zoom:67%"><p>出现这些提示就表示成功了，运行完毕后会发现项目根目录出现了我们在pipelines.py中设置好的MyData.json。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408222346.jpg" alt="2020-04-08_222333" style="zoom:67%"></li><li><p>打开MyData.json看一下，Perfect！</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408222427.jpg" alt="2020-04-08_222417" style="zoom:67%"><p>有了这个json文件，我们就可以利用Python的pandas、numpy等工具进行各种处理，然后用matplotlib等模块进行画图了。</p></li></ol><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>本文只是对Python的scrapy爬虫进行了简单的介绍和用一个小Demo讲述了如何使用scrapy爬取网页数据，希望对你有所帮助。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
            <tag> 数据处理 </tag>
            
            <tag> 学堂在线 </tag>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux上机实战1：正则表达式</title>
      <link href="/2020/03/31/Linux%E4%B8%8A%E6%9C%BA%E4%BD%9C%E4%B8%9A1%EF%BC%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BA%94%E7%94%A8%20%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/"/>
      <url>/2020/03/31/Linux%E4%B8%8A%E6%9C%BA%E4%BD%9C%E4%B8%9A1%EF%BC%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BA%94%E7%94%A8%20%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<p><strong>本文是关于Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式应用的一个样例，获取北京某时刻PM2.5的数据，然后进行处理，输出到csv文件中，并画图表展示。</strong></p><a id="more"></a><h2 id="一、题目要求："><a href="#一、题目要求：" class="headerlink" title="一、题目要求："></a>一、题目要求：</h2><p>​ <strong>从因特网上搜索 Web 页，用 wget 获取网页，处理网页 html 文本数据，从中提取出当前时间点北京各监测站的 PM2.5 浓度，输出如下 CSV 格式数据：</strong><br><strong><em>2020 03 09 13:00:00, 海淀区万柳 ,73<br>2020 03 09 13:00:00, 昌平镇 ,67<br>2020 03 09 13:00:00, 奥体中心 ,66<br>2020 03 09 13:00:00, 海淀区万柳 ,73<br>2020 03 09 13:00:00, 昌平镇 ,73<br>2020 03 09 13:00:00, 奥体中心 ,75</em></strong><br>​ <strong>撰写实验报告，要求：写出对数据的分析和处理思路，列出各个处理步骤并给出解释。</strong></p><h2 id="二、详细步骤："><a href="#二、详细步骤：" class="headerlink" title="二、详细步骤："></a>二、详细步骤：</h2><ol><li><p>从因特网上搜索 Web 页，找到与含有北京各监测站的 PM2.5 浓度的网站，我找到了<strong><em>绿色呼吸网（<a href="http://www.pm25.com/city/beijing.html）" target="_blank" rel="external nofollow noopener noreferrer">http://www.pm25.com/city/beijing.html）</a></em></strong>，网站如下：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203352.jpg" alt="1" style="zoom:33%"></li><li><p>使用<strong><em>Xshell</em></strong>登录到Ubuntu服务器：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203420.jpg" alt="2" style="zoom:33%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203429.jpg" alt="3" style="zoom:33%"></li><li><p>使用<strong><em>wget</em></strong>命令获取该网页：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.pm25.com/city/beijing.html</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203525.jpg" alt="4" style="zoom:80%"></li><li><p>使用<strong><em>cat</em></strong>命令查看该网页的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203554.jpg" alt="5" style="zoom:80%"><p>我们关注的内容：</p><p>​ ①数据更新的时间：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203608.jpg" alt="6" style="zoom:67%"><p>​ ②各监测点PM2.5浓度数据：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203617.jpg" alt="7" style="zoom:50%"></li><li><p>发现时间的地方有个<strong><em>“更新时间：”</em></strong>，监测点名称的地方都有<strong><em>“pjadt_location”</em></strong>，而PM2.5浓度的地方都有<strong><em>“pjadt_pm25”</em></strong>。根据这个特性，先使用<strong><em>awk</em></strong>命令将需要的行保留下来。下面先进行编写<strong><em>1.awk</em></strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim 1.awk</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203629.jpg" alt="8"></p><p>运行以下命令，对所需行进行过滤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203639.jpg" alt="9" style="zoom:50%"><p>发现除了我们想要的行还多出了这几行：</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203656.jpg" alt="10"></p><p>经过观察，发现<strong><em>“PM2.5”</em></strong>浓度这一行与我们所需的行的区别是，我们所需的行有<strong><em>μg</em></strong>，而<strong><em>“PM2.5”</em></strong>浓度这一行没有：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203705.jpg" alt="11" style="zoom:50%"><p>我们重新对<strong><em>“1.awk”</em></strong>进行编辑，直接将<strong><em>“监测站点”</em></strong>这一行排除，并且对<strong><em>“PM2.5”</em></strong>浓度这一行采用额外的过滤规则:</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203718.jpg" alt="12"></p><p>重新运行以下命令，对所需行进行过滤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203729.jpg" alt="13" style="zoom:50%"><p>发现已经筛选出了所需要的行。</p></li><li><p>现在再利用<strong><em>sed</em></strong>命令将<strong><em>html标签”&lt;&gt;“</em></strong>中的内容和<strong><em>“更新时间：”</em></strong>这个无用的信息删除：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203744.jpg" alt="14" style="zoom:50%"><p>发现已经只剩余我们想要的数据。</p></li><li><p>但是这些数据不在同一行，且没有明显特征，无法进行<strong><em>awk</em></strong>命令，因此先使用<strong><em>tr</em></strong>命令将这些行合并为一行，以空格分隔：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203757.jpg" alt="15" style="zoom:50%"></li><li><p>最后利用<strong><em>awk</em></strong>命令将所需内容规格化输出即可，先编辑<strong><em>“2.awk”</em></strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim 2.awk</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203807.jpg" alt="16" style="zoom:80%"><p>利用<strong><em>for</em></strong>循环输出，<strong><em>NF</em></strong>为列数，刚好循环$$(NF-2)/4$$次，第一个<strong><em>%s</em></strong>为<strong><em>日期</em></strong>，第二个<strong><em>%s</em></strong>为<strong><em>时间</em></strong>，第三个<strong><em>%s</em></strong>为<strong><em>监测点名称</em></strong>，第四个<strong><em>%s</em></strong>为<strong><em>PM2.5浓度</em></strong>，再运行以下<strong><em>awk</em></strong>命令，即可得到格式化的输出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | awk -f 2.awk  | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203819.jpg" alt="17" style="zoom:50%"><p>发现输出已经符合题目要求。</p></li><li><p>将结果重定向到文件<strong><em>“beijing.csv”</em></strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | awk -f 2.awk  &gt; beijing.csv</span><br><span class="line">vim beijing.csv</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203839.jpg" alt="18" style="zoom:50%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203853.jpg" alt="19" style="zoom:80%"></li><li><p>将<strong><em>beijing.csv</em></strong>发送到电脑，并将编码转为<strong><em>ANSI</em></strong>：</p></li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203903.jpg" alt="20" style="zoom:67%"><ol start="11"><li><p>由于不同时间的数据有所不同，因此我的过滤语句考虑了不同时间点的情况<strong><em>（北京一共有12个监测点，有时有的监测点没有数据，因此是只有11个监测点的数据）</em></strong>，直接运行以下命令就可以直接将数据导出为<strong><em>beijing.csv</em></strong>，以下为另一时间点的数据情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.pm25.com/city/beijing.html</span><br><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | awk -f 2.awk  &gt; beijing.csv</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/placeholder.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203916.jpg" alt="21" style="zoom:67%"></li></ol><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>本文以一个样例，详细介绍了Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式的相关知识，希望对你的Linux学习有所帮助。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> 上机实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 实战 </tag>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello Blog</title>
      <link href="/2020/03/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2020/03/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p class="p center large">HELLO BLOG</p><p>欢迎来到我的博客，这是我的第一篇博客测试文章。</p><a id="more"></a><p>如果有什么问题，欢迎到我的Github提问我。</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> first </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
