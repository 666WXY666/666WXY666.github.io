<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux MOOC习题 5~9章</title>
      <link href="/2020/04/29/Linux-MOOC%E4%B9%A0%E9%A2%98-5~9%E7%AB%A0/"/>
      <url>/2020/04/29/Linux-MOOC%E4%B9%A0%E9%A2%98-5~9%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的，接上文<a href="https://666wxy666.github.io/2020/04/28/Linux-MOOC习题-1~4章/">Linux MOOC习题 1~4章</a>。</p><a id="more"></a><p><strong><em>PS：第七章是上机实验，第一次上机实验详见我的另一篇文章：<a href="https://666wxy666.github.io/2020/03/31/Linux-上机实战1-正则表达式/">Linux 上机实战1 正则表达式</a>，第二次的实验因为是提交阶段暂不公开，后面可能会发。</em></strong></p><h2 id="五、文件管理和目录管理"><a href="#五、文件管理和目录管理" class="headerlink" title="五、文件管理和目录管理"></a>五、文件管理和目录管理</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429100515.jpg" alt="2020-04-28_212515" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429101043.jpg" alt="2020-04-28_212629" style="zoom:80%"><p>在Linux中，Shell会自动帮你进项一些替换，实际运行的命令其实是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp src/x.c src/xx.c src/xxx.c src/x.h src/xx.h src/xxx.h……</span><br></pre></td></tr></table></figure><p>如果有好多.c和.h文件（超过两个），cp就会报错：</p><div class="note"><p>cp: target xxx is not a directory</p></div><p>但是如果只有两个.c或.h文件，那么就会产生非常严重的后果，实际运行了下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp src/x.c src/y.c</span><br></pre></td></tr></table></figure><p>这是非常危险的，这样y.c的内容会被x.c覆盖，这是我们不愿意看到的。</p><p>那么应该用什么命令来实现题目的要求呢？</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp src/*.[ch] .</span><br></pre></td></tr></table></figure><p>我们显示地指定复制到‘.’（当前目录），就不会出现问题了。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429102232.jpg" alt="2020-04-28_212656" style="zoom:80%"><p>这也是在Linux中很怪的地方，明明你不能写这个文件，但是你却可以删除，如果不加-f，删除只读文件时会有提示，但是加了-f就没有任何提示了。还有一点需要注意的是，如第6题所说，加了-f也不能删除无权限删除的文件。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429102450.jpg" alt="2020-04-28_212714" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429102603.jpg" alt="2020-04-28_212737" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429102659.jpg" alt="2020-04-28_212800" style="zoom:80%"><p>关于第10题，不像Windows，在Linux中这些后缀都是约定俗成的，只是为了做标记用，好区分而已，没有实际意义。</p><h2 id="六、Linux命令风格和文件系统"><a href="#六、Linux命令风格和文件系统" class="headerlink" title="六、Linux命令风格和文件系统"></a>六、Linux命令风格和文件系统</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429102845.jpg" alt="2020-04-28_213047" style="zoom:80%"><p>这两个题都忽略了“符号链接”的作用。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429103011.jpg" alt="2020-04-28_213114" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429103030.jpg" alt="2020-04-28_213125" style="zoom:80%"><p>这俩没啥好说的。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429103227.jpg" alt="2020-04-28_213133" style="zoom:80%"><p>在Linux中，一般是-1代表失败，&gt;=0的值代表成功。</p><h2 id="八、文件和目录的权限、Shell"><a href="#八、文件和目录的权限、Shell" class="headerlink" title="八、文件和目录的权限、Shell"></a>八、文件和目录的权限、Shell</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429103454.jpg" alt="2020-04-28_213341" style="zoom:80%"><p>目录有执行权限（即x权限）意味着分析路径名过程中可检索该目录。</p><h2 id="九、替换、元字符和转义"><a href="#九、替换、元字符和转义" class="headerlink" title="九、替换、元字符和转义"></a>九、替换、元字符和转义</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429103639.jpg" alt="2020-04-28_213427" style="zoom:80%"><p>如果你是执行这个命令的操作员，估计你马上就心情不好了。由于拼错了单词，把DATABASE不小心写成了DATEBASE，灾难来了。未命名的变量被bash替换为空字符串，实际上你以root身份执行了最邪恶的一条命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /*</span><br></pre></td></tr></table></figure><p>永远不要盲目自信，谁也保证不了自己不会犯错误，怎么才能避免这样的悲剧发生？</p><ol><li><p>尽量不要以root身份登录。</p></li><li><p>bash有选项，引用未定义的变量会出错而不是替换为空字符串，可以打开这个选项。早期的编程语言把引用的未定义过的变量自动加上定义，这种做法实际上太糟糕，最早的FORTRAN语言就这种做法，据说曾因此导致一次太空任务失败。</p></li><li><p>在设计你自己的系统时，建库命令能够从目录名开始建库，就是说要求建库之前/opt/data下不需要存在目录puma，而不是要求建库之前/opt/data有个空目录puma，这样的话，即使你的命令变成了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf $DATEBASE</span><br></pre></td></tr></table></figure><p>因为你的失误，会导致rm命令抱怨缺少参数而什么都不做，这个结果是可以接受的。</p></li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429103652.jpg" alt="2020-04-28_213530" style="zoom:80%"><p>在Linux中，目前我已知的Shell的元字符有：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">空格 制表符 回车 &gt; &lt; | ; &amp; $ * [ ] ? \ ( ) '' " " 反撇号`</span><br></pre></td></tr></table></figure><p>第11题中，这三个命令都可以取消文件通配符*的特殊作用，让echo直接打印字符*，并且这个替换是Shell进行的，也就是说，echo拿到的命令都是一样的，他是分不清操作员输入的是哪个命令，这三个命令的参数：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429105050.jpg" alt="2020-04-29_105040" style="zoom:80%"><p>而对于不转义的命令的参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo *</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429105236.jpg" alt="2020-04-29_105224" style="zoom:80%"><p>因此结果就很明显了：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429104247.jpg" alt="2020-04-29_104233" style="zoom:80%"><details><summary><p>未完待续……</p></summary><div class="content"><p>所有的习题：</p><p><a href="https://666wxy666.github.io/2020/04/28/Linux-MOOC习题-1~4章/">Linux MOOC习题 1~4章</a></p><p><a href="https://666wxy666.github.io/2020/04/29/Linux-MOOC习题-5~9章/">Linux MOOC习题 5~9章</a></p></div></details>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> 习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 习题 </tag>
            
            <tag> 易错点 </tag>
            
            <tag> 网课 </tag>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux MOOC习题 1~4章</title>
      <link href="/2020/04/28/Linux-MOOC%E4%B9%A0%E9%A2%98-1~4%E7%AB%A0/"/>
      <url>/2020/04/28/Linux-MOOC%E4%B9%A0%E9%A2%98-1~4%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自己随便整理了一下在学习Linux网课时遇到的一些习题，易错点之类的</p><a id="more"></a><p>废话不多说，直接开始：</p><p><strong><em>PS：第一章是课程介绍。</em></strong></p><h2 id="二、开始使用Linux和文本文件的处理"><a href="#二、开始使用Linux和文本文件的处理" class="headerlink" title="二、开始使用Linux和文本文件的处理"></a>二、开始使用Linux和文本文件的处理</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428215021.jpg" alt="2020-04-28_211643" style="zoom:80%"><p>第1题其实是很基础的问题，早期的终端一般由键盘、显示器和RS232串行通信接口构成，没有磁盘存储器，其实可以想象成和打字机差不多的东西，他就是直接在显示器上显示了，不需要磁盘存储器。</p><p>第2题也比较基础：</p><ul><li><p>行律的作用是：</p><ul><li><p>一行内字符的缓冲、回显和编辑，直到按下回车键；</p></li><li><p>数据的加工，类似第二题中的将“\n”替换为“\r\n”；</p></li><li><p>将CTRL-C字符转换为终止进程的信号；</p></li></ul></li><li><p>驱动程序其实是串口与行律的接口，负责上行和下行字符流。</p></li></ul><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428215752.jpg" alt="2020-04-28_211738" style="zoom:80%"><p>这俩题没啥好说的，基础知识。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428215946.jpg" alt="2020-04-28_211810" style="zoom:80%"><p>并不是这样的，CTRL-C确实传送了字符<strong><em>（Ctrl+字母组合键可以产生ASCII码为1-26的控制字符，字母序号是几，ASCII码就是几，Ctrl+C的ASCII码应为3）</em></strong>，其实还是行律的作用，他将CTRL-C字符转换为终止进程的信号，从而通知Linux主机，进程终止。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428220440.jpg" alt="2020-04-28_211920" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428220658.jpg" alt="2020-04-28_211921" style="zoom:80%"><p>这俩题都是关于uniq命令的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uniq -u</span><br><span class="line">uniq -d</span><br><span class="line">uniq</span><br></pre></td></tr></table></figure><p>第一个命令，-u代表unique，只打印没有重复的行；</p><p>第二个命令，-d代表duplicated，只打印重复的行，注意重复的行只打印一次；</p><p>第三个命令，啥也不加就是都打印，但是也是复的行只打印一次；</p><p>还有就是关于重复的行，意思其实是连续的紧邻的两行内容相同才被叫做重复的行，因此第10题是错的。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428221248.jpg" alt="2020-04-28_211949" style="zoom:80%"><p>这个题不难，写出来的目的是只要记得</p><div class="note"><p>Less is more</p></div><p>就很好想了，less是more的升级版。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428221932.jpg" alt="2020-04-28_212008" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428221942.jpg" alt="2020-04-28_212044" style="zoom:80%"><p>这俩题也不多解释了，od不可打印字符也可以显示，tr是用于翻译，把string1出现的字符替换为string2中对应的字符，ASCII字符0也可以翻译。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tr string1 string2</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429095034.jpg" alt="2020-04-28_212253" style="zoom:80%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429095238.png" alt="7992DB2C9EF7FC68967A6E866A101062" style="zoom:50%"><h2 id="三、正则表达式"><a href="#三、正则表达式" class="headerlink" title="三、正则表达式"></a>三、正则表达式</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200428222324.jpg" alt="2020-04-28_212126" style="zoom:80%"><p>先说一下正则表达式的<strong><em>元字符</em></strong>，有6个，分别是：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">. * [ \ ^ $</span><br></pre></td></tr></table></figure><p>关于他们各自的含义不多说，自行百度，一定要注意的是]不是元字符，很容易搞错。</p><p>而关于单字符正则表达式，显而易见，就是匹配一个字符呗，不过有特殊的:</p><ul><li><p>\加一个字符构成的转义字符，看做单字符正则表达式；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\. \* \[ \\ \^ \$</span><br></pre></td></tr></table></figure></li><li><p>[]定义的集合也被看做单字符正则表达式;</p></li></ul><p>因为.本来的含义是匹配任意字符，转义后就是单字符.，因此都是单字符正则表达式，而$在尾部时和^在首部时有特殊含义，因此转义之前不是，转义后就是单字符​了。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429091111.jpg" alt="2020-04-28_212154" style="zoom:80%"><p>这个题就比较有意思了，先看一下sed命令的基本用法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed 's/正则表达式/字符串/g' 文件名列表</span><br></pre></td></tr></table></figure><p>s选项的意思是替换，将文件中匹配第一个//中的正则表达式的内容替换为第二个//中的字符串。</p><p>然后我们在看上题‘’中的正则表达式（也就是第一个//中的内容）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\[[^][]*]</span><br></pre></td></tr></table></figure><p>可以分为三部分：</p><ol><li><p>因为[是元字符，加\转义后就变成了真正的[。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\[</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>中间这一部分是一个集合[]，重复一次或多次[]*，集合里面的^代表排除，排除了]和[，意思就是说除]和[外的字符重复一次或多次。那么这就有一个问题，第一个[为什么不与第一个]匹配，反而去和最后一个]匹配呢？个人觉得，因为如果与第一个]匹配，就有了[^]，这显然是错误的正则表达式，那么就继续向后匹配，因此就匹配到了最后面的]。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[^][]*</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>因为]不是元字符，也没有[与它匹配，它就是单纯的一个字符]。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">]</span><br></pre></td></tr></table></figure></li></ol><p>综上所述，这个正则表达式可以匹配“[非]和[的字符任意多个]”，类似[参考文献23]这样的，但是如果[]里面还有]和[，就匹配不到了。再看上面的题，第二个//里面是空的，那么就是把匹配到的内容删除，很好理解。</p><h2 id="四、文件比较，文件通配符"><a href="#四、文件比较，文件通配符" class="headerlink" title="四、文件比较，文件通配符"></a>四、文件比较，文件通配符</h2><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429093945.jpg" alt="2020-04-28_212239" style="zoom:80%"><p>vi的基本用法，在命令状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">:q退出（:quit的缩写）</span><br><span class="line">:q!退出且不保存（:quit!的缩写）</span><br><span class="line">:wq保存并退出</span><br><span class="line">:wq!保存并退出即使文件没有写入权限（强制保存退出）</span><br><span class="line">:x保存并退出（类似:wq，但是只有在有更改的情况下才保存）</span><br><span class="line">:exit保存并退出（和:x相同）</span><br><span class="line">:qa退出所有(:quitall的缩写)</span><br><span class="line">:cq退出且不保存（即便有错误）</span><br></pre></td></tr></table></figure><p>另外在“正常模式”下输入“ZZ”来保存并退出Vim（和:x相同），或者“ZQ”不保存并退出（和:q!相同）注意此处ZZ大写和小写是完全不同的。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200429095046.jpg" alt="2020-04-28_212356" style="zoom:80%"><p>diff一般用于比较文本文件。</p><details><summary><p>未完待续……</p></summary><div class="content"><p>所有的习题：</p><p><a href="https://666wxy666.github.io/2020/04/28/Linux-MOOC习题-1~4章/">Linux MOOC习题 1~4章</a></p><p><a href="https://666wxy666.github.io/2020/04/29/Linux-MOOC习题-5~9章/">Linux MOOC习题 5~9章</a></p></div></details>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> 习题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 习题 </tag>
            
            <tag> 易错点 </tag>
            
            <tag> 网课 </tag>
            
            <tag> MOOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python爬虫小Demo：学堂在线课程</title>
      <link href="/2020/04/07/Python%E7%88%AC%E8%99%AB%E5%B0%8FDemo/"/>
      <url>/2020/04/07/Python%E7%88%AC%E8%99%AB%E5%B0%8FDemo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>本文是关于Python中的Spider的小Demo，通过Python的scrapy爬取京学堂在线课程的相关数据。</strong></p><a id="more"></a><h2 id="一、编译环境："><a href="#一、编译环境：" class="headerlink" title="一、编译环境："></a>一、编译环境：</h2><p>PyCharm 2019.3.4 (Professional Edition)</p><p>Build #PY-193.6911.25, built on March 18, 2020</p><p>Runtime version: 11.0.6+8-b520.43 amd64</p><p>VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o</p><p>Windows 10 10.0</p><p>GC: ParNew, ConcurrentMarkSweep</p><p>Memory: 725M</p><p>Cores: 8</p><p>Registry: ide.balloon.shadow.size=0</p><p>Non-Bundled Plugins:</p><p>GrepConsole,Statistic,cn.yiiguxing.plugin.translate,com.chrisrm.idea.MaterialThemeUI,com.notime.intellijPlugin.backgroundImagePlus,com.wakatime.intellij.plugin,izhangzhihao.rainbow.brackets,mobi.hsz.idea.gitignore, net.vektah.codeglance, org.intellij.gitee</p><p>Python Version：3.7（Anaconda3）</p><p>Package：</p><p>scrapy==2.0.1</p><h2 id="二、详细步骤"><a href="#二、详细步骤" class="headerlink" title="二、详细步骤"></a>二、详细步骤</h2><h3 id="①准备工作"><a href="#①准备工作" class="headerlink" title="①准备工作"></a>①准备工作</h3><ol><li>在Pycharm中新建一个Pure Python项目（记得要按照一中的要求配好Python环境）。</li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200407214757.jpg" alt="2020-04-07_214715" style="zoom:67%"><ol start="2"><li>打开Pycharm的下方的终端（Terminal），当然这些也可以在系统终端里操作，不过可能需要的步骤多一些，还是直接在Pycharm里方便一些。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408164806.png" alt="2020-04-08_164744"></p><ol start="3"><li><p>在终端里输入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myScrapy</span><br></pre></td></tr></table></figure><p>本来是可以在我们刚刚创建的项目里新建一个名为“myScrapy”的scrapy项目的，但是不知道为什么竟然报错了：</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408190345.png" alt="2020-04-08_190330"></p><p>这是什么奇奇怪怪的错误，我都没有“d:\bld\scrapy_1584555997548_h_env\python.exe”这个目录，经过查找相关问题的资料，问题可能是出在Python环境上，因为我上一次新建项目时并没有报错，这一次我直接用的上一个项目的环境，网上还有一种说法是Scrapy的bug，详见</p><p><a href="https://github.com/scrapy/scrapy/issues/4289" target="_blank" rel="external nofollow noopener noreferrer">Fatal error launching scrapy&gt;1.6.0 from Anaconda Prompt</a></p><p><a href="https://github.com/conda-forge/scrapy-feedstock/issues/37" target="_blank" rel="external nofollow noopener noreferrer">Issue with conda-forge scrapy&gt;1.6.0 on Windows</a></p><p>目前找到了两种解决方案：</p><ol><li><p>把python环境复制到报错的那个目录（d:\bld\scrapy_1584555997548_h_env\python.exe），然后在创建scrapy项目，但这个解决方法有点愚蠢，就没有采用。</p></li><li><p>在scrapy命令前面添加“python -m”选项：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m scrapy startproject myScrapy</span><br></pre></td></tr></table></figure><p>就可以正常创建了。</p><p>关于Python的-m选项，官方给出的解释是“run library module as a script”，简单来说就是将库中的Python模块当作脚本去运行。</p><p>特别感谢简书的大佬<a href="https://www.jianshu.com/p/323fc9a1d7d2" target="_blank" rel="external nofollow noopener noreferrer">ccw1078</a>提供的解释，很清晰明了，有兴趣的可以去瞅一下，因为和本文的主题爬虫没啥关系，在这里就不赘述了。</p></li></ol></li></ol><ol start="4"><li><p>出现这些提示就代表创建成功了。</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408192211.jpg" alt="2020-04-08_192151"></p><p>然后在spiders文件里新建一个spider.py文件，用于写爬虫。</p></li><li><p>我们来看一下目前scrapy项目的目录结构。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408203358.jpg" alt="2020-04-08_203310" style="zoom:67%"><p>__init__.py：pycharm生成的文件，简化导入语句用的，可以忽略，没啥用，建议删了，留着可能会出问题。</p><p>spiders：存放你Spider爬虫源文件</p><p>​ spider.py：代码主要在这里写。</p><p>items.py：数据容器。</p><p>middlewares.py：Downloader Middlewares(下载器中间件)和Spider Middlewares(蜘蛛中间件)实现的地方。</p><p>pipelines.py：项目管道文件，相当于数据中转站。实现数据的清洗，储存，验证。</p><p>settings.py：scrapy的全局配置。</p><p>scrapy.cfg：配置文件。</p><p>scrapy已经帮我们把大体框架写好了，我们主要要修改的文件是spider.py，items.py，pipelines.py，settings.py。</p></li><li><p>这是爬虫spider的基本工作方式，想要深入了解的可以去网上查找资料。</p></li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408201638.png" alt="scrapy" style="zoom:33%"><h3 id="②开始写代码"><a href="#②开始写代码" class="headerlink" title="②开始写代码"></a>②开始写代码</h3><ol><li><p>先来写items.py。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyscrapyItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    school = scrapy.Field()</span><br><span class="line">    num = scrapy.Field()</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>再来搞settings.py，这个只需要找到这个注释掉的语句，把#去掉就OK了，就像这样：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408210722.jpg" alt="2020-04-08_210710" style="zoom:67%"></li><li><p>pipelines.py就很好写了，基本可以当模板来用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyscrapyPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 这个就是爬虫生成的文件，可以支持好多种格式，这里使用的是json文件</span></span><br><span class="line">            self.file = open(<span class="string">'MyData.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> err:</span><br><span class="line">            print(err)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        dict_item = dict(item)</span><br><span class="line">        json_str = json.dumps(dict_item, ensure_ascii=<span class="literal">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(json_str)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure></li><li><p>spider.py是我们主要写的核心部分。这里需要一些html的xpath相关知识来对项进行定位，可以自行查找相关资料。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">@Copyright: Copyright (c) 2020 苇名一心 All Rights Reserved.</span></span><br><span class="line"><span class="string">@Project: xuetangzaixian</span></span><br><span class="line"><span class="string">@Description: </span></span><br><span class="line"><span class="string">@Version: </span></span><br><span class="line"><span class="string">@Author: 苇名一心</span></span><br><span class="line"><span class="string">@Date: 2020-04-08 20:31</span></span><br><span class="line"><span class="string">@LastEditors: 苇名一心</span></span><br><span class="line"><span class="string">@LastEditTime: 2020-04-08 20:31</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> myScrapy.myScrapy.items <span class="keyword">import</span> MyscrapyItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mySpider</span><span class="params">(scrapy.spiders.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># spider的名字</span></span><br><span class="line">    name = <span class="string">"xuetang"</span></span><br><span class="line">    <span class="comment"># 限制spider爬取的域名</span></span><br><span class="line">    allowed_domains = [<span class="string">"www.xuetangx.com/"</span>]</span><br><span class="line"><span class="comment"># 爬虫要爬取的网页，是一个列表，按顺序爬取</span></span><br><span class="line">    start_urls = [<span class="string">"http://www.xuetangx.com/partners"</span>]</span><br><span class="line"><span class="comment"># 这是一种方式，可以爬取网页中所有的项</span></span><br><span class="line">    <span class="comment"># def parse(self, response):</span></span><br><span class="line">    <span class="comment">#     item = MyscrapyItem()</span></span><br><span class="line">    <span class="comment">#     for each in response.xpath("/html/body/article[1]/section/ul/*"):</span></span><br><span class="line">    <span class="comment">#         item['school'] = each.xpath("a/div[2]/h3/text()").extract()</span></span><br><span class="line">    <span class="comment">#         item['num'] = each.xpath("a/div[2]/p[1]/text()").extract()</span></span><br><span class="line">    <span class="comment">#         if item['num']:</span></span><br><span class="line">    <span class="comment">#             item['num'] = re.findall(r'\d+', item['num'][0])</span></span><br><span class="line">    <span class="comment">#         if item['school'] and item['num']:</span></span><br><span class="line">    <span class="comment">#             yield (item)</span></span><br><span class="line">    <span class="comment"># 这是第二种方式，使用for循环，制定爬取项的数目</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        item = MyscrapyItem()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">144</span>):</span><br><span class="line">            item[<span class="string">'school'</span>] = response.xpath \</span><br><span class="line">                (<span class="string">"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/h3/text()"</span>.format(i)).extract()</span><br><span class="line">            item[<span class="string">'num'</span>] = response.xpath \</span><br><span class="line">                (<span class="string">"/html/body/article[1]/section/ul/li[&#123;&#125;]/a/div[2]/p[1]/text()"</span>.format(i)).extract()</span><br><span class="line">            <span class="comment"># 判断爬取的项目是否为空，把非空的项目提交</span></span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'school'</span>] <span class="keyword">and</span> item[<span class="string">'num'</span>]:</span><br><span class="line">                <span class="keyword">yield</span> (item)</span><br></pre></td></tr></table></figure></li></ol><h3 id="③可以开始运行啦"><a href="#③可以开始运行啦" class="headerlink" title="③可以开始运行啦"></a>③可以开始运行啦</h3><ol><li><p>在运行前要先在项目根目录下建立一个begin.py文件来控制scrapy爬虫的运行。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408212615.jpg" alt="2020-04-08_212604" style="zoom:80%"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"><span class="comment"># "xuetang"是我们上面spider.py中定义的爬虫名</span></span><br><span class="line">cmdline.execute(<span class="string">"scrapy crawl xuetang"</span>.split())</span><br></pre></td></tr></table></figure></li><li><p>最终的项目结构（__init__.py没啥用，删了）：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408212844.jpg" alt="2020-04-08_212834" style="zoom:67%"></li><li><p>运行begin.py就可以开始爬虫了。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408212955.jpg" alt="2020-04-08_212946" style="zoom:67%"><p>出现这些提示就表示成功了，运行完毕后会发现项目根目录出现了我们在pipelines.py中设置好的MyData.json。</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408222346.jpg" alt="2020-04-08_222333" style="zoom:67%"></li><li><p>打开MyData.json看一下，Perfect！</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200408222427.jpg" alt="2020-04-08_222417" style="zoom:67%"><p>有了这个json文件，我们就可以利用Python的pandas、numpy等工具进行各种处理，然后用matplotlib等模块进行画图了。</p></li></ol><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>本文只是对Python的scrapy爬虫进行了简单的介绍和用一个小Demo讲述了如何使用scrapy爬取网页数据，希望对你有所帮助。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
            <tag> 数据处理 </tag>
            
            <tag> 学堂在线 </tag>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 上机实战1 正则表达式</title>
      <link href="/2020/03/31/Linux-%E4%B8%8A%E6%9C%BA%E5%AE%9E%E6%88%981-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2020/03/31/Linux-%E4%B8%8A%E6%9C%BA%E5%AE%9E%E6%88%981-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>本文是关于Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式应用的一个样例，获取北京某时刻PM2.5的数据，然后进行处理，输出到csv文件中，并画图表展示。</strong></p><a id="more"></a><h2 id="一、题目要求："><a href="#一、题目要求：" class="headerlink" title="一、题目要求："></a>一、题目要求：</h2><p>​ <strong>从因特网上搜索 Web 页，用 wget 获取网页，处理网页 html 文本数据，从中提取出当前时间点北京各监测站的 PM2.5 浓度，输出如下 CSV 格式数据：</strong><br><strong><em>2020 03 09 13:00:00, 海淀区万柳 ,73<br>2020 03 09 13:00:00, 昌平镇 ,67<br>2020 03 09 13:00:00, 奥体中心 ,66<br>2020 03 09 13:00:00, 海淀区万柳 ,73<br>2020 03 09 13:00:00, 昌平镇 ,73<br>2020 03 09 13:00:00, 奥体中心 ,75</em></strong><br>​ <strong>撰写实验报告，要求：写出对数据的分析和处理思路，列出各个处理步骤并给出解释。</strong></p><h2 id="二、详细步骤："><a href="#二、详细步骤：" class="headerlink" title="二、详细步骤："></a>二、详细步骤：</h2><ol><li><p>从因特网上搜索 Web 页，找到与含有北京各监测站的 PM2.5 浓度的网站，我找到了<strong><em>绿色呼吸网（<a href="http://www.pm25.com/city/beijing.html）" target="_blank" rel="external nofollow noopener noreferrer">http://www.pm25.com/city/beijing.html）</a></em></strong>，网站如下：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203352.jpg" alt="1" style="zoom:33%"></li><li><p>使用<strong><em>Xshell</em></strong>登录到Ubuntu服务器：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203420.jpg" alt="2" style="zoom:33%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203429.jpg" alt="3" style="zoom:33%"></li><li><p>使用<strong><em>wget</em></strong>命令获取该网页：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.pm25.com/city/beijing.html</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203525.jpg" alt="4" style="zoom:80%"></li><li><p>使用<strong><em>cat</em></strong>命令查看该网页的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203554.jpg" alt="5" style="zoom:80%"><p>我们关注的内容：</p><p>​ ①数据更新的时间：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203608.jpg" alt="6" style="zoom:67%"><p>​ ②各监测点PM2.5浓度数据：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203617.jpg" alt="7" style="zoom:50%"></li><li><p>发现时间的地方有个<strong><em>“更新时间：”</em></strong>，监测点名称的地方都有<strong><em>“pjadt_location”</em></strong>，而PM2.5浓度的地方都有<strong><em>“pjadt_pm25”</em></strong>。根据这个特性，先使用<strong><em>awk</em></strong>命令将需要的行保留下来。下面先进行编写<strong><em>1.awk</em></strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim 1.awk</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203629.jpg" alt="8"></p><p>运行以下命令，对所需行进行过滤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203639.jpg" alt="9" style="zoom:50%"><p>发现除了我们想要的行还多出了这几行：</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203656.jpg" alt="10"></p><p>经过观察，发现<strong><em>“PM2.5”</em></strong>浓度这一行与我们所需的行的区别是，我们所需的行有<strong><em>μg</em></strong>，而<strong><em>“PM2.5”</em></strong>浓度这一行没有：</p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203705.jpg" alt="11" style="zoom:50%"><p>我们重新对<strong><em>“1.awk”</em></strong>进行编辑，直接将<strong><em>“监测站点”</em></strong>这一行排除，并且对<strong><em>“PM2.5”</em></strong>浓度这一行采用额外的过滤规则:</p><p><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203718.jpg" alt="12"></p><p>重新运行以下命令，对所需行进行过滤：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203729.jpg" alt="13" style="zoom:50%"><p>发现已经筛选出了所需要的行。</p></li><li><p>现在再利用<strong><em>sed</em></strong>命令将<strong><em>html标签”&lt;&gt;“</em></strong>中的内容和<strong><em>“更新时间：”</em></strong>这个无用的信息删除：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203744.jpg" alt="14" style="zoom:50%"><p>发现已经只剩余我们想要的数据。</p></li><li><p>但是这些数据不在同一行，且没有明显特征，无法进行<strong><em>awk</em></strong>命令，因此先使用<strong><em>tr</em></strong>命令将这些行合并为一行，以空格分隔：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203757.jpg" alt="15" style="zoom:50%"></li><li><p>最后利用<strong><em>awk</em></strong>命令将所需内容规格化输出即可，先编辑<strong><em>“2.awk”</em></strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim 2.awk</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203807.jpg" alt="16" style="zoom:80%"><p>利用<strong><em>for</em></strong>循环输出，<strong><em>NF</em></strong>为列数，刚好循环$$(NF-2)/4$$次，第一个<strong><em>%s</em></strong>为<strong><em>日期</em></strong>，第二个<strong><em>%s</em></strong>为<strong><em>时间</em></strong>，第三个<strong><em>%s</em></strong>为<strong><em>监测点名称</em></strong>，第四个<strong><em>%s</em></strong>为<strong><em>PM2.5浓度</em></strong>，再运行以下<strong><em>awk</em></strong>命令，即可得到格式化的输出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | awk -f 2.awk  | more</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203819.jpg" alt="17" style="zoom:50%"><p>发现输出已经符合题目要求。</p></li><li><p>将结果重定向到文件<strong><em>“beijing.csv”</em></strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | awk -f 2.awk  &gt; beijing.csv</span><br><span class="line">vim beijing.csv</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203839.jpg" alt="18" style="zoom:50%"> <img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203853.jpg" alt="19" style="zoom:80%"></li><li><p>将<strong><em>beijing.csv</em></strong>发送到电脑，并将编码转为<strong><em>ANSI</em></strong>：</p></li></ol><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203903.jpg" alt="20" style="zoom:67%"><ol start="11"><li><p>由于不同时间的数据有所不同，因此我的过滤语句考虑了不同时间点的情况<strong><em>（北京一共有12个监测点，有时有的监测点没有数据，因此是只有11个监测点的数据）</em></strong>，直接运行以下命令就可以直接将数据导出为<strong><em>beijing.csv</em></strong>，以下为另一时间点的数据情况：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.pm25.com/city/beijing.html</span><br><span class="line">cat beijing.html | awk -f 1.awk | sed -e 's/&lt;[^&lt;&gt;]*&gt;//g' -e 's/更新时间：//g' | tr '\n' ' ' | awk -f 2.awk  &gt; beijing.csv</span><br></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/666WXY666/cdn/img/placeholder/large.svg" data-original="https://gitee.com/wxy_666/images/raw/master/20200331203916.jpg" alt="21" style="zoom:67%"></li></ol><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>本文以一个样例，详细介绍了Linux中的文本处理三剑客（grep，sed，awk），以及正则表达式的相关知识，希望对你的Linux学习有所帮助。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> 上机实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 实战 </tag>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello Blog</title>
      <link href="/2020/03/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2020/03/19/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p class="p center large">HELLO BLOG</p><p>欢迎来到我的博客，这是我的第一篇博客测试文章。</p><a id="more"></a><p>如果有什么问题，欢迎到我的Github提问我。</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> first </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
